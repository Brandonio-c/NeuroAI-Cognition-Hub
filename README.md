# NeuroAI-Cognition-Hub
## Neuro-Symbolic AI and Cognition Links

Welcome to the Neuro-Symbolic AI and Cognition Links repository. This repository is a curated collection of resources, links, and references related to the exciting field of neuro-symbolic artificial intelligence, with a specific focus on cognition within AI. Whether you're a researcher, student, or AI enthusiast, this repository aims to provide valuable information and references to help you dive deeper into this evolving field.  Dive into the world of cognitive models, architectures, and AI systems that aim to simulate human-like thinking processes. Stay up-to-date with the latest developments and research in this cutting-edge domain, and contribute to our growing repository to help advance the field even further. Join us in the exploration of the fascinating intersection of neuroscience, symbolic reasoning, and AI cognition.

## Table of Contents

1.  [About Neuro-Symbolic AI and the Common Model of Cognition](#about-neuro-symbolic-ai)
2.  [Cognition in AI - 2024 synopsis](#cognition-in-ai)
3.  [Featured](#featured)
4.  [Survey papers](#Survey-Papers)
5.  [Symbolic Language](#Symbolic-Language)
6.  [Symbolic Reasoning AI major projects](#Symbolic-Reasoning-AI-major-projects)
7.  [Knowledge representation major projects](#Knowledge-representation-major-projects)
8.  [Cognitive Architectures and Generative Models](#Cognitive-Architectures-and-Generative-Models)
9.  [Common Model of Cognition](#Common-Model-of-Cognition)
10.  [Memory AI major projects](#Memory-AI-major-projects)
11.  [Meta-level control major projects](#Meta-level-control-major-projects)
12. [Benchmarks](#benchmarks)
13. [Generative AI Impactful Projects](#Generative-AI-Impactful-Projects)
14. [Useful AI tools (2024) - There's literally an AI for everything](#useful-ai-tools-2024---theres-literally-an-ai-for-everything)
15. [Links to other useful GitHub pages](#links-to-other-useful-github-pages)
16. [Usage](#usage)
17. [Contributing](#contributing)
18. [License](#license)

Up to week 10 day 00 (just started this week)

## About Neuro-Symbolic AI

Neuro-symbolic AI is an interdisciplinary approach that combines symbolic reasoning with neural networks to create more advanced and intelligent AI systems. This repository will provide links to research papers, articles, and tools that explore this integration.

## Cognition in AI

Cognition within AI is an essential aspect of building machines that can think and understand like humans. We've gathered resources related to cognitive models, cognitive architectures, and AI systems that simulate human-like thinking processes.

### The Grounding Problem 

### The Common Model of Cognition

## Featured 

- [![Website](https://img.shields.io/badge/Website-Deep_Learning_Is_Hitting_A_Wall-red)](https://nautil.us/deep-learning-is-hitting-a-wall-238440/) - "Deep Learning Is Hitting a Wall" by Gary Marcus.

- [![GitHub](https://img.shields.io/badge/Website-OpenCog-red)](https://github.com/opencog) - Visit the GitHub repository for OpenCog.

- [![Website](https://img.shields.io/badge/Website-IHMC-red)](https://www.ihmc.us/publications-on-explainable-ai/) - Publications on Explainable AI

## Survey Papers

| Publication Year | Name | Description | Paper Link | GitHub Link | Summary |
| ---------------- | ---- | ----------- | ---------- | ----------- | ------- |
| [2023](https://arxiv.org/abs/2111.08164) | NeuralSym | A Survey on Neural-symbolic Learning Systems | [![arXiv ](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2111.08164) | N/A | <details><summary>Summary</summary>"A Survey on Neural-symbolic Learning Systems" examines the combination of neural networks and symbolic AI. It covers the AI evolution, integration challenges, and methods like learning for reasoning, reasoning for learning, and a joint approach. The survey highlights efficiency, generalization, interpretability improvements, diverse applications, and future research directions in AI.<br>- Discusses neural-symbolic system integration<br>- Explores AI evolution, challenges, and methodologies</details> |
| [2023](https://arxiv.org/abs/2305.08876) | NeuroTax | Neurosymbolic AI and its Taxonomy: a survey | [![arXiv ](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2305.08876) | N/A | <details><summary>Summary</summary>"Neurosymbolic AI and its Taxonomy: A Survey" explores the integration of neural networks with symbolic AI towards AGI. It covers AI evolution, knowledge representation, learning and reasoning processes, and emphasizes the importance of explainability and trustworthiness in AI. The survey analyzes various neurosymbolic models, their applications, and suggests future research directions, offering an overview of neurosymbolic AI's advancements, methodologies, and prospects.<br>- Discusses AI evolution and complexity<br>- Analyzes neurosymbolic models and applications</details> |
| [2023](https://arxiv.org/abs/2012.05876) | NeuroWave | Neurosymbolic AI: The 3rd Wave | [![arXiv ](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2012.05876) | N/A | <details><summary>Summary</summary>"Neurosymbolic AI: The 3rd Wave" provides a comprehensive analysis of the merging of neural networks and symbolic AI. Highlighting the field's evolution, it discusses current debates and technical aspects of neurosymbolic computing. The paper stresses the need for integrating learning and reasoning, efficiency, scalability, and trust in AI. It compares different models and discusses the AAAI 2020 conference's role in advancing neurosymbolic AI. The paper calls for leveraging both symbolic and neural approaches for more advanced AI systems.<br>- Emphasizes integration of neural and symbolic AI<br>- Discusses AI challenges and model comparisons</details> |
| [2023](https://arxiv.org/abs/2003.00330) | GNN-NSC Survey | Graph Neural Networks Meet Neural-Symbolic Computing | [![arXiv ](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2003.00330) | N/A | <details><summary>Summary</summary>This survey explores the integration of Graph Neural Networks (GNNs) with Neural-Symbolic Computing (NSC). It delves into various GNN models, their application in relational learning, reasoning, and combinatorial optimization. The paper emphasizes GNNs' role in efficiency, scalability, and real-world applications, addressing integration challenges and future research directions. It highlights the importance of explainability and trust in AI systems, offering a comprehensive view of GNNs' potential in NSC.<br>- Discusses GNN models and applications<br>- Explores integration challenges and future directions</details> |
| [2021](https://arxiv.org/abs/2109.06133) | NSAI | Neuro-Symbolic AI: An Emerging Class of AI Workloads and their Characterization | [![arXiv ](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2109.06133) | [![GitHub](https://img.shields.io/github/stars/vacancy/NSCL-PyTorch-Release.svg?style=social)](https://github.com/vacancy/NSCL-PyTorch-Release), [![GitHub](https://img.shields.io/github/stars/chuangg/CLEVRER.svg?style=social)](https://github.com/chuangg/CLEVRER), [![GitHub](https://img.shields.io/github/stars/facebookresearch/d2go.svg?style=social)](https://github.com/facebookresearch/d2go), [![GitHub](https://img.shields.io/github/stars/OpenNMT/OpenNMT-py.svg?style=social)](https://github.com/OpenNMT/OpenNMT-py), [![GitHub](https://img.shields.io/github/stars/YunzhuLi/PropNet.svg?style=social)](https://github.com/YunzhuLi/PropNet), [![GitHub](https://img.shields.io/github/stars/google/neural-logic-machines.svg?style=social)](https://github.com/google/neural-logic-machines) | <details><summary>Summary</summary>Neuro-symbolic AI (NSAI) represents a novel integration of traditional rules-based AI approaches with modern deep learning techniques, offering advancements in image and video reasoning while reducing the need for extensive training data. This paper provides an in-depth analysis of three distinct NSAI models: the Neuro-Symbolic Concept Learner (NSCL), Neuro-Symbolic Dynamic Reasoning (NS-DR), and Neural Logic Machines (NLM). While NSCL and NS-DR are composed of several submodels including image/video parsers and symbolic executors, NLM functions as an end-to-end model. The analysis reveals that NSAI models generally exhibit less potential for parallelism compared to traditional neural models, primarily due to their complex control flow and operations such as scalar multiplication. Data movement is highlighted as a potential bottleneck, similar to other machine learning workloads. The paper categorizes the operations within NSAI models into eight types for performance analysis, suggesting that while the neural components often dominate, there are opportunities for acceleration, especially in handling low-operational-intensity operations.</details> |


## Symbolic Language

| Publication Year | Name | Description | Paper Link | GitHub Link | Summary |
| ---------------- | ---- | ----------- | ---------- | ----------- | ------- |
| [2023](https://arxiv.org/abs/2309.16467) | CPG | Code property graph framework for software analysis | [![arXiv ](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2309.16467) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/IBM/cpg) | <details><summary>Summary</summary>The "Compositional Program Generator" (CPG) is a novel neuro-symbolic architecture designed for efficient language processing, particularly in few-shot learning scenarios. Unlike conventional neural networks, which struggle with systematic generalization and require extensive data, CPG excels in learning new concepts with minimal examples. Its core strengths lie in three attributes: modularity, where it uses specialized modules for different semantic functions; composition, allowing the combination of modules for varied input types; and abstraction, employing grammar rules for consistent input processing. CPG's innovative approach is showcased through its remarkable performance on standard benchmarks like SCAN and COGS, achieving state-of-the-art results with drastically fewer data samples. This efficiency is facilitated by its curricular training method, which incrementally adjusts to varying sentence lengths and complexities. CPG's ability to retrain efficiently for new concepts or grammar adaptations, without forgetting previous learnings, indicates its potential for broader applications in systematic language generalization tasks.</details> |
| [2002-22](https://propbank.github.io/) | PropBank | PropBank (proposition bank) approach to semantic role labeling over the last two decades | [![Paper](https://img.shields.io/badge/Paper-ACL'22-blue)](https://aclanthology.org/2022.starsem-1.24/) | [![GitHub](https://img.shields.io/github/stars/propbank/propbank.svg?style=social)](https://github.com/propbank/) | <details><summary>Summary</summary>PropBank, evolving over twenty years, has significantly expanded its scope, encompassing non-verbal predicates such as adjectives, prepositions, and multi-word expressions, as well as a broader range of domains, genres, and languages. This expansion enhances the testing and generalization capabilities of semantic role labeling (SRL) systems. PropBank's methodology, focusing on semantic role labeling, has transitioned from relying on syntactic parses to richer semantic representations. A key component of PropBank is its Frames, housing rolesets with predicate argument structures, now essential to various meaning representations like AMR and UMR. Recent developments include the inclusion of non-verbal predicates and an extensive overhaul of the lexicon to support domain-specific annotation projects such as Spatial AMR and the THYME project. These adaptations have increased PropBank's utility and relevance in diverse NLP applications. PropBank's enhanced web presence, user-friendly tools, and accessibility improvements, such as utility scripts and a searchable frame files website, have made it a more powerful resource for researchers and practitioners in the field.</details> |
| [2017-22](https://universalpropositions.github.io/) | Universal PropBank | Annotate text in different languages with a layer of "universal" semantic role labeling annotation | [![Website](https://img.shields.io/badge/Website-See_Publications-blue)](https://universalpropositions.github.io/) | [![GitHub](https://img.shields.io/github/stars/UniversalPropositions/UniversalPropBank.svg?style=social)](https://github.com/UniversalPropositions) | <details><summary>Summary</summary>The Universal Proposition Banks (UP) project aims to annotate texts in various languages with "universal" semantic role labeling, using English Proposition Bank's frame and role labels. UP2.0, an enhancement over v1.0, offers higher quality PropBanks using advanced monolingual SRL and improved annotation auto-generation. It expands language coverage from 7 to 23 and introduces span annotation for syntactic analysis decoupling, along with Gold data for some languages. UP2.0 is built on the Universal Dependency Treebanks release 2.9, utilizing English PropBank version 3.0 labels. The project focuses on annotating verbs with English frames, excluding auxiliary verbs, and aims to label about 90% of all verbs in each language. The annotations, mostly model-predicted, vary in quality due to domain differences. UP facilitates research in multilingual and cross-lingual SRL, with applications in advanced NLP and IBM products. Future work includes adding new languages and improving annotation quality. UP2.0 provides a valuable resource for expanded shallow semantic parsing and semantic role labeling research and applications.</details> |
| [2012-18](https://verbs.colorado.edu/verbnet/) | VerbNet | The largest online network of English verbs that links their syntactic and semantic patterns | [![Website](https://img.shields.io/badge/Website-VerbNet-red)](https://verbs.colorado.edu/verbnet/) | [![GitHub](https://img.shields.io/github/stars/cu-clear/verbnet.svg?style=social)](https://github.com/cu-clear/verbnet) | <details><summary>Summary</summary>VerbNet (VN) is the largest online English verb lexicon, providing a hierarchical, domain-independent verb classification system. It extends and refines Levin's classes for syntactic and semantic coherence. Each class includes thematic roles, selectional restrictions, and frames combining syntactic descriptions with semantic predicates and temporal functions. VN integrates Levin's work with extensions by Korhonen and Briscoe, expanding its coverage, including verbs taking various complements. The integration enriches VN, increasing its classes, thematic roles, semantic predicates, and syntactic restrictions. This comprehensive Levin-style classification is essential for creating training corpora for syntactic parsers and semantic role labelers. VN's thematic role assignments and class memberships facilitate large-scale experimentation in syntax-based class utility for improving NLP tools.</details> |
| [2021](https://aclanthology.org/2021.iwcs-1.21/) | IWCS21 | Semantic linking in computational linguistics | [![Paper](https://img.shields.io/badge/Paper-2021-blue)](https://aclanthology.org/2021.iwcs-1.21/) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/cu-clear/semlink) | <details><summary>Summary</summary>SemLink serves as a vital bridge connecting lexical semantic resources like PropBank, VerbNet, FrameNet, and WordNet. It enables the use of each resource's unique features, thereby enhancing semantic analysis capabilities. Recent updates have focused on automatic and manual enhancements to maintain consistency across evolving resources, alongside the introduction of sense embeddings and subject/object information. SemLink's size has nearly doubled through these updates, significantly improving its coverage. Essential for research in lexical semantics, word sense disambiguation, and semantic role labeling, SemLink continues to evolve, with future efforts aimed at filling gaps through manual annotation and evaluating the utility of linked resources</details> |
| [2019](https://aclanthology.org/N19-1334.pdf) | Wilcox19 | Structural analysis in natural language processing | [![Paper](https://img.shields.io/badge/Paper-N19--1334-blue)](https://aclanthology.org/N19-1334/) | N/A | <details><summary>Summary</summary>This study investigates whether training language models with hierarchical structure supervision improves their ability to learn non-local grammatical dependencies, previously only examined for subject-verb agreement. By comparing LSTM models with two structurally supervised models – Recurrent Neural Network Grammars (RNNGs) and a version of Parsing-as-Language-Modeling – the research focuses on two grammatical dependencies: Negative Polarity licensing and Filler–Gap Dependencies. Findings reveal that structurally supervised models significantly outperform traditional LSTMs, particularly in RNNGs, which show superior understanding of complex grammatical rules. These results highlight the benefits of structural supervision in language model training, especially for complex grammatical learning </details> |
| [2017](https://users.cs.utah.edu/~lifeifei/papers/deeplog.pdf) | DeepLog | Anomaly detection in system logs through deep learning | [![acm ](https://img.shields.io/badge/Paper-2017-blue)](https://dl.acm.org/doi/10.1145/3133956.3134015) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/Thijsvanede/DeepLog) | <details><summary>Summary</summary>DeepLog leverages a deep neural network, specifically Long Short-Term Memory (LSTM), to analyze system logs for anomaly detection. It treats logs as natural language sequences, learning normal patterns for identifying deviations. Uniquely, DeepLog continually updates its model to adapt to new log patterns over time, enhancing its relevance and accuracy. The system is capable of diagnosing anomalies by constructing workflows from logs, offering insights into potential root causes. DeepLog proves effective in various settings, including HDFS and OpenStack logs, outperforming traditional data mining methods in detecting both execution path and parameter value anomalies, showcasing its versatility and dynamic adaptability.</details> |
| [2015](https://aclanthology.org/Q15-1034/) | Semantic Proto-roles | Semantic proto role linking model | [![Paper](https://img.shields.io/badge/Paper-Q15--1034-blue)](https://aclanthology.org/Q15-1034/) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/aaronstevenwhite/SemanticProtoRoleLinkingModel?tab=readme-ov-file) | <details><summary>Summary</summary>The study presents a large-scale corpus-based validation of Dowty's proto-role theory, proposing a shift from traditional categorical roles to a property-based annotation in understanding semantic relationships. It harnesses crowdsourcing to gather data on proto-agent and proto-patient properties across sentences in the PropBank corpus. Analyzing 11 proto-role properties, the study reveals significant role fragmentation with a wide array of unique property configurations. This approach highlights the nuanced nature of semantic roles, challenging existing frameworks like VerbNet and FrameNet. The findings offer significant insights for applications in semantic parsing and role labeling, with future expansion to other languages and broader linguistic analysis anticipated. </details> |
| [2003](https://ids-pub.bsz-bw.de/frontdoor/deliver/index/docId/5416/file/Johnson_Petruck_Baker_Ellsworth_Ruppenhofer_Fillmore_FrameNet_Theory_and_Practice_2003.pdf) | FrameNet | Linguistic theory and practice analysis | [![Paper](https://img.shields.io/badge/Paper-2003-blue)](https://ids-pub.bsz-bw.de/frontdoor/deliver/index/docId/5416/file/Johnson_Petruck_Baker_Ellsworth_Ruppenhofer_Fillmore_FrameNet_Theory_and_Practice_2003.pdf) | [![Website](https://img.shields.io/badge/Website-FRAMENET-red)](https://framenet.icsi.berkeley.edu/) | <details><summary>Summary</summary>FrameNet is a linguistic project that catalogs the semantic frames of English words, focusing on the roles and relationships between words in sentences. It's based on Frame Semantics, a theory positing that the meaning of a word is best understood in the context of a larger conceptual structure, or "frame". FrameNet provides structured representations of word meanings, identifying the elements and participants involved in different scenarios. This resource is valuable for natural language processing applications like semantic role labeling and text understanding, offering insights into how language constructs meaning through interrelated words and their contextual roles. </details> |
| [1993](https://aclanthology.org/J93-2004/) | Penn Treebank | Linguistic data consortium analysis | [![Paper](https://img.shields.io/badge/Paper-J93--2004-blue)](https://aclanthology.org/J93-2004/) | N/A | <details><summary>Summary</summary>The Penn Treebank project undertakes the ambitious task of building a large annotated corpus of American English, aiming to significantly progress in understanding both written text and spoken language. It involves annotating over 4.5 million words with part-of-speech and skeletal syntactic structure. The POS tagging utilizes a simplified tagset and combines automatic assignment with human correction, ensuring speed, consistency, and accuracy. Bracketing, following a similar process, employs the automatic parser Fidditch for initial analysis. The project's output, crucial for linguistic research and natural language processing, plans future enhancements for more detailed annotations and addressing complexities in linguistic structures. </details> |



## Symbolic Reasoning AI major projects


| Publication Year | Name | Description | Paper Link | GitHub Link | Summary |
| ---------------- | ---- | ----------- | ---------- | ----------- | ------- |
| [2024](https://openreview.net/forum?id=NzjyY2Z9zJd) | neuro symbolic text game | A Hybrid Neuro-Symbolic approach for Text-Based Games using Inductive Logic Programming | [![Paper](https://img.shields.io/badge/Paper-2024-blue)](https://openreview.net/forum?id=NzjyY2Z9zJd) | N/A | <details><summary>Summary</summary>This paper presents a hybrid neuro-symbolic architecture for Text-Based Games (TBGs), combining symbolic reasoning with neural reinforcement learning. It uses inductive logic programming to learn symbolic rules as default theories with exceptions, enabling non-monotonic reasoning in partially observable environments. The approach employs WordNet for rule generalization, enhancing adaptability to unseen objects and scenarios. The architecture features a context encoder, action encoder, neural and symbolic action selectors, and a symbolic rule learner, with priority given to symbolic reasoning. The model outperforms traditional methods in TBGs, showing potential for future improvements in action selection and agent adaptability </details> |
| [2024](https://openreview.net/forum?id=ORAhay0H4x) |Plan-SOFAI | Plan-SOFAI: A Neuro-Symbolic Planning Architecture | [![Paper](https://img.shields.io/badge/Paper-2024-blue)](https://openreview.net/forum?id=ORAhay0H4x) | NA | <details><summary>Summary</summary> Plan-SOFAI introduces a neuro-symbolic architecture for AI planning inspired by Kahneman's cognitive theory. It integrates fast (System-1) and slow (System-2) thinking models, utilizing System-1 for quick solutions based on past experiences and System-2 for logical, reasoned approaches. A metacognitive module oversees solver selection, balancing speed and accuracy. Focused on classical planning problems, Plan-SOFAI demonstrates versatility and efficiency in various testing scenarios, outperforming traditional methods in balancing solving speed and solution optimality. The architecture's adaptability allows integration of new techniques, promising broader applications beyond classical planning. Future efforts aim to enhance Plan-SOFAI's capabilities and explore new domains of application.</details> |
| [2023](https://arxiv.org/abs/2308.04445) | Cyc by Cycorp | Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc | [![arXiv 2023](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2308.04445) | [![Star](https://img.shields.io/github/stars/cycorp/cyc.svg?style=social)](https://github.com/cycorp) | <details><summary>Summary</summary>**10 Point Summary:**<br>1. Addresses limitations of LLMs in trustworthiness and reasoning.<br>2. Proposes integration with symbolic AI, using Cyc as an example.<br>3. Combines strengths of LLMs and symbolic AI.<br>4. Discusses 16 elements necessary for trustworthy AI.<br>5. Emphasizes need for explicit knowledge representation.<br>6. Highlights importance of reasoning, world models, higher-order logic.<br>7. Cyc's common-sense knowledge can enhance LLMs.<br>8. Suggests improved explanation, deduction, induction capabilities.<br>9. Aims to overcome LLMs' shortcomings in complex language.<br>10. Envisions more reliable, interpretable AI systems.<br><br>**Detailed Summary:**<br>The 2023 paper "Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc" by Doug Lenat and Gary Marcus discusses the limitations of current Large Language Models (LLMs) in trustworthiness and reasoning. It proposes integrating LLMs with symbolic AI systems like Cyc to address these limitations. The paper outlines 16 key elements necessary for a trustworthy AI, emphasizing the need for explicit knowledge, reasoning, world models, and higher-order logic capabilities. It suggests that integrating LLMs with systems like Cyc, which can handle complex reasoning and possess extensive common-sense knowledge, could lead to more reliable and interpretable AI systems. This approach aims to overcome the shortcomings of LLMs, particularly in areas like explanation, deduction, induction, and handling of complex language structures.</details> |
| [2023](https://arxiv.org/abs/2312.03905) | PseudoSL | Research on Pseudo Supervised Learning | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2312.03905) | [![GitHub](https://img.shields.io/github/stars/UCLA-StarAI/PseudoSL.svg?style=social)](https://github.com/UCLA-StarAI/PseudoSL) | <details><summary>Summary</summary>The paper introduces a novel Pseudo-Semantic Loss for Autoregressive Models to incorporate logical constraints into deep learning training processes. Addressing the computational complexity of maximizing symbolic constraint likelihoods in expressive distributions like transformers, the approach employs a pseudolikelihood-based approximation around a model sample. This innovation ensures the efficient computation of neuro-symbolic losses. Empirically validated across diverse tasks like Sudoku solving, Warcraft path prediction, and language model detoxification, the method significantly improves the production of logically consistent outputs and reduces language model toxicity. This work extends neuro-symbolic learning, combining symbolic knowledge representation with neural network capabilities, and enhancing model reliability and explainability. </details> |
| [2023](https://arxiv.org/abs/2302.14207) | SemStreng | Study on Semantic Strengthening | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2302.14207) | [![GitHub](https://img.shields.io/github/stars/UCLA-StarAI/Semantic-Strengthening.svg?style=social)](https://github.com/UCLA-StarAI/Semantic-Strengthening) | <details><summary>Summary</summary>The paper introduces "Semantic Strengthening of Neuro-Symbolic Learning," addressing the computational challenges in neuro-symbolic methods. It iteratively strengthens an approximation by focusing on the most relevant constraints, measured through conditional mutual information. This process ensures better alignment of gradients between constraint distributions, enhancing the model's accuracy in structured-output tasks. The approach efficiently computes mutual information using tractable circuits and maintains sound probabilistic semantics. Evaluated on complex tasks like Warcraft path prediction, Sudoku solving, and MNIST matching, the method significantly improves prediction accuracy. This work combines neural networks' feature extraction prowess with symbolic reasoning, offering a scalable solution for advanced neuro-symbolic learning. </details> |
| [2023](https://arxiv.org/abs/2306.03553) | ARC | Abstraction and Reasoning Challenge | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2306.03553) | [![GitHub](https://img.shields.io/github/stars/fchollet/ARC.svg?style=social)](https://github.com/fchollet/ARC), [![GitHub](https://img.shields.io/github/stars/tanchongmin/ARC-Challenge.svg?style=social)](https://github.com/tanchongmin/ARC-Challenge) | <details><summary>Summary</summary>The paper discusses using GPT-4 for solving the ARC Challenge, emphasizing learning from limited samples. It contrasts this with traditional deep learning's reliance on extensive data. The method involves reverse engineering instructions from input-output pairs and applying these to test inputs. The research highlights the importance of self-supervised learning in understanding complex structures and demonstrates LLMs' capabilities in zero-shot and few-shot learning. It also explores chain-of-thought prompting for multi-step reasoning. The study underscores the role of human-like biases in learning and suggests improvements like multi-agent systems and memory usage for better performance, aiming to solve a majority of ARC tasks with GPT-4. </details> |
| [2023](https://aclanthology.org/W13-2322.pdf) | AMR (3.0) | Abstract Meaning Representation (Bank) | [![Conference](https://img.shields.io/badge/Conference-ACL'23-blue)](https://aclanthology.org/W13-2322.pdf) | [![Website](https://img.shields.io/badge/Website-AMR-red)](https://amr.isi.edu/) | <details><summary>Summary</summary>Abstract Meaning Representation (AMR) is a semantic representation language capturing the essence of English sentences to advance natural language understanding and generation. It offers a unified format for semantically annotating sentences, inspired by the success of syntactic treebanks. AMR's graph-based structure, emphasizing readability and simplicity, abstracts from syntax to focus on meaning. Utilizing tools like a power editor and the 'smatch' script, AMR ensures consistent and efficient annotation. With an expanding AMR bank and applications in diverse areas including machine translation, AMR is poised to significantly impact the field, driving future research and practical applications in natural language processing. </details> |
| [2023](https://doi.org/10.35111/44cy-bp51) | AMR (3.0) | Abstract Meaning Representation (AMR) Annotation Release 3.0 | [![Conference](https://img.shields.io/badge/Conference-Related'23-blue)](https://doi.org/10.35111/44cy-bp51) | [![LDC Catalog](https://img.shields.io/badge/LDC_Catalog-Website-red)](https://catalog.ldc.upenn.edu/LDC2020T02) | <details><summary>Summary</summary>Abstract Meaning Representation (AMR) Annotation Release 3.0, developed by Linguistic Data Consortium and partners, contains a sembank of over 59,255 English sentences from various sources for advancing natural language processing. This release enhances quality, adds annotations, and includes multi-sentence annotations. AMR represents sentence meanings as graphs, abstracting from syntax to focus on semantic structure. The data, sourced from diverse programs and including new text types, is split into training, development, and test partitions. This release is part of ongoing efforts to enrich machine translation and other applications through improved semantic understanding. /details> |
| [2023](https://arxiv.org/abs/2301.10414) | Logic & Info | Towards a Unification of Logic and Information Theory | [![arXiv 2023](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2301.10414) | N/A | <details><summary>Summary</summary>This work explores the efficient transmission of logical statements, innovatively connecting logic and information theory. It treats logical statements as equivalent if they lead to the same deductions, applying rate-distortion theory for communication. Using propositional logic modeled as polynomial equations in finite fields, it investigates various scenarios, including transmissions with and without pre-existing background statements. Key contributions include demonstrating the optimality of incremental communications and the surprising "less is more" theorems. The study also delves into the partition compression problem, providing theoretical bounds and proposing linear codes for practical implementation, achieving asymptotic optimality under certain conditions. </details> |
| [2023](https://arxiv.org/abs/2312.12891) | MinePlnr | Benchmark for long-horizon planning in Minecraft worlds | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2312.12891) | [![GitHub](https://img.shields.io/github/stars/IretonLiu/mine-pddl.svg?style=social)](https://github.com/IretonLiu/mine-pddl/) | <details><summary>Summary</summary> MinePlanner introduces a benchmark for testing AI planning capabilities in large, complex Minecraft worlds. Comprising 45 tasks with escalating difficulty, it assesses planners' abilities to navigate environments dense with objects, many of which are irrelevant to task goals. The benchmark challenges current planners, including Fast Downward and ENHSP-20, revealing significant limitations in handling large-domain problems. MinePlanner highlights the need for advancements in planning algorithms to cope with real-world scales and complexity. This framework not only serves as a testbed for AI planning research but also aims to foster collaboration between the learning and planning sectors in AI. </details> |
| [2023](https://openreview.net/forum?id=PgDbFx9bk8) | GenPlan23 | Abstract world models for value-preserving planning | [![Paper](https://img.shields.io/badge/Paper-2023-blue)](https://openreview.net/forum?id=PgDbFx9bk8) | N/A | <details><summary>Summary</summary> Learning Abstract World Models for Value-preserving Planning by Rafael Rodriguez-Sanchez and George Konidaris explores the development of abstract Markov Decision Processes (MDPs) for advanced planning in reinforcement learning. The paper addresses the challenge of complex decision-making in general-purpose agents by proposing state and action abstractions within abstract MDPs. This approach ensures effective planning and decision-making while maintaining compatibility with specific tasks. By leveraging information maximization and deep learning methods, the paper demonstrates improved planning efficiency in various environments. The research contributes significantly to the field by blending theoretical and empirical insights for effective abstract model learning and planning. </details> |
| [2023](https://openreview.net/forum?id=PjYUlfpLJE) | D3A | Building long-term 3D semantic maps | [![Paper](https://img.shields.io/badge/Paper-2023-blue)](https://openreview.net/forum?id=PjYUlfpLJE) | [![GitHub](https://img.shields.io/github/stars/IfrahIdrees/D3A.svg?style=social)](https://github.com/IfrahIdrees/D3A) | <details><summary>Summary</summary>Introduces an algorithm for building dynamic 3D semantic maps, crucial for household robots in unstructured environments.</details> |
| [2023](https://openreview.net/forum?id=VD0ksRTljb) | IBM-LNN | Logical Neural Networks for reasoning | [![Paper](https://img.shields.io/badge/Paper-2023-blue)](https://openreview.net/forum?id=VD0ksRTljb) | [![GitHub](https://img.shields.io/github/stars/IBM/LNN.svg?style=social)](https://github.com/IBM/LNN) | <details><summary>Summary</summary>Focuses on the development and application of Logical Neural Networks for advanced reasoning tasks.</details> |
| [2023](https://aclanthology.org/2023.acl-short.57/) | NS Proprioception | Neuro-Symbolic World Models with Conversational Proprioception | [![Paper](https://img.shields.io/badge/Paper-2023-blue)](https://aclanthology.org/2023.acl-short.57/) | N/A | <details><summary>Summary</summary>Explores neuro-symbolic world models to improve explainability in natural language interactions, using Logical Neural Networks in TextWorld.</details> |
| [2023](https://arxiv.org/abs/2110.10973) | IBM-LOA | Logical Optimal Actions for Text-based Games | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2110.10973) | [![GitHub](https://img.shields.io/github/stars/ibm/loa.svg?style=social)](https://github.com/ibm/loa) | <details><summary>Summary</summary>Presents the LOA framework integrating neuro-symbolic approach in reinforcement learning for text-based interaction games.</details> |
| [2023](https://arxiv.org/abs/2110.10963) | NS-RL | Neuro-Symbolic Reinforcement Learning with First-Order Logic | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2110.10963) | N/A | <details><summary>Summary</summary>Introduces a method for reinforcement learning in text-based games using a neuro-symbolic approach with first-order logic.</details> |
| [2023](https://www.ijcai.org/proceedings/2023/0839.pdf) | Plansformer tool | Plansformer Tool: Demonstrating Generation of Symbolic Plans Using Transformers | [![Paper](https://img.shields.io/badge/Paper-2023-blue)](https://www.ijcai.org/proceedings/2023/0839.pdf) | N/A | <details><summary>Summary</summary>"Plansformer Tool: Demonstrating Generation of Symbolic Plans Using Transformers" introduces Plansformer, a novel AI tool leveraging transformer-based language models to generate symbolic plans. Fine-tuned on classical planning domains, Plansformer demonstrates effective plan generation, evaluated through metrics like ROUGE and BLEU, and validated for optimality. It surpasses other models in generating valid plans for simple domains but faces challenges in more complex scenarios. Offering a user-friendly web interface, Plansformer enhances planning efficiency in diverse fields. The research opens new pathways for utilizing large language models in symbolic domains, signifying a step forward in AI planning technologies. </details> |
| [2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/d0241a0fb1fc9be477bdfde5e0da276a-Abstract-Conference.html) | compositional generalisation | Compositional generalization through abstract representations in human and artificial neural networks | [![Paper](https://img.shields.io/badge/Paper-2022-blue)](https://proceedings.neurips.cc/paper_files/paper/2022/hash/d0241a0fb1fc9be477bdfde5e0da276a-Abstract-Conference.html) | N/A | <details><summary>Summary</summary>This study explores compositional generalization in humans and artificial neural networks (ANNs) through a highly compositional task. By examining human behavior and neural correlates using fMRI, it identifies behavioral patterns of compositional generalization. The study also incorporates pretraining paradigms in ANNs, embedding prior knowledge of the task to improve performance. Results indicate that pretraining induces abstract internal representations in ANNs, leading to enhanced generalization and learning efficiency. It also reveals a content-specific topography of abstract representations across the human cortex. This research provides empirical evidence for the role of abstract representations in compositional generalization, with implications for ANN design and training. </details> |
| [2022](https://arxiv.org/abs/2212.08681) | Plansformer | Plansformer: Generating Symbolic Plans using Transformers | [![arXiv](https://img.shields.io/badge/arXiv-2022-brightgreen)](https://arxiv.org/abs/2212.08681) | N/A | <details><summary>Summary</summary>Introduces Plansformer, an LLM fine-tuned on planning problems, demonstrating high performance in various planning domains.</details> |
| [2022](https://arxiv.org/abs/2212.12050) | Semantic NS computing | A Semantic Framework for Neural-Symbolic Computing | [![arXiv 2022](https://img.shields.io/badge/arXiv-2022-brightgreen)](https://arxiv.org/abs/2212.12050) | N/A | <details><summary>Summary</summary>Simon Odense and Artur d’Avila Garcez's paper, "A Semantic Framework for Neural-Symbolic Computing," presents an innovative framework integrating neural networks and symbolic artificial intelligence. This integration addresses the limitations of both approaches, proposing a standard for encoding symbolic knowledge into neural networks. It covers key aspects like semantic encoding, probabilistic models, and semantic regularization, contributing significantly to explainable AI. The framework enables a unified understanding and comparison of diverse neural-symbolic methods. Although it faces challenges like expressive limitations, this pioneering work lays the groundwork for future advancements in neural-symbolic computing, a crucial step towards more advanced, interpretable AI systems. </details> |
| [2022](https://arxiv.org/abs/2211.08451) | Kogito | A Knowledge-Grounded Neural Conversational Model | [![arXiv](https://img.shields.io/badge/arXiv-2022-brightgreen)](https://arxiv.org/abs/2211.08451) | [![GitHub](https://img.shields.io/github/stars/epfl-nlp/kogito.svg?style=social)](https://github.com/epfl-nlp/kogito) | <details><summary>Summary</summary>"kogito" by Mete Ismayilzada and Antoine Bosselut is an innovative toolkit designed to generate commonsense knowledge inferences from textual input. It integrates with natural language generation models, offering a customizable and extensible interface for inference generation. The toolkit includes a library of pre-trained models like GPT-2, GPT-3, and COMET, and features modules for head extraction, relation matching, and inference filtering. kogito allows users to define custom knowledge relations, enhancing its adaptability. Accompanied by extensive documentation, the tool aims to standardize and simplify the process of generating meaningful commonsense inferences, paving the way for more intelligent and context-aware AI systems. </details> |
| [2022](https://ieeexplore.ieee.org/document/9488275) | RoboCat | A Category Theoretic Framework for Robotic Interoperability Using Goal-Oriented Programming | [![IEEE](https://img.shields.io/badge/Conference-IEEE'22-blue)](https://ieeexplore.ieee.org/document/9488275) | N/A | <details><summary>Summary</summary> RoboCat, developed by Angeline Aguinaldo and colleagues, introduces a novel framework for robotic programming, leveraging category theory for enhanced interoperability and usability. It transforms the way robots are programmed by adopting a goal-oriented, declarative approach that utilizes high-level abstractions rather than focusing on low-level, vendor-specific programming. By defining hierarchical interfaces and using mathematical representations, RoboCat simplifies integrating new hardware and software into robotic systems. This framework not only eases the programming process but also ensures consistency across different robotic platforms, making it a significant step towards more adaptable and efficient robotic applications in various industries. </details> |
| [2022](https://arxiv.org/abs/2203.15827) | LinkBERT | Pretraining Language Models with Document Links | [![arXiv 2022](https://img.shields.io/badge/arXiv-2022-brightgreen)](https://arxiv.org/abs/2203.15827) | [![Star](https://img.shields.io/github/stars/michiyasunaga/LinkBERT.svg?style=social)](https://github.com/michiyasunaga/LinkBERT) | <details><summary>Summary</summary>"LinkBERT: Pretraining Language Models with Document Links" by Michihiro Yasunaga, Jure Leskovec, and Percy Liang proposes a novel pretraining approach for language models, utilizing document links to capture inter-document relationships. LinkBERT, by integrating these links, effectively enhances language understanding and reasoning capabilities, especially in tasks requiring multi-hop reasoning and document relation comprehension. Tested in both general and biomedical domains, LinkBERT demonstrates notable improvements over traditional models like BERT, particularly in complex tasks and few-shot learning scenarios. This approach sets new benchmarks in biomedical NLP tasks, showcasing the immense potential of document-link-based pretraining in language models. </details> |
| [2021](https://arxiv.org/abs/2109.12240) | LCN | Logical Credal Networks | [![arXiv 2021](https://img.shields.io/badge/arXiv-2021-brightgreen)](https://arxiv.org/abs/2109.12240) | [![Star](https://img.shields.io/github/stars/radum2275/crema.svg?style=social)](https://github.com/radum2275/crema) | <details><summary>Summary</summary> "Logical Credal Networks" by Haifeng Qian et al. presents a novel approach to probabilistic logic modeling, addressing the challenge of representing imprecise information in AI applications. LCNs allow for the use of arbitrary logic formulas with probability bounds, overcoming the limitations of traditional models in expressiveness and dependency representation. Featuring a unique Markov condition, LCNs can manage cyclic dependencies, making them versatile for real-world scenarios like Mastermind puzzles and credit card fraud detection. The experimental results demonstrate LCNs' superiority in aggregating diverse information sources, marking a significant advancement in probabilistic logic models and their application in uncertain environments. </details> |
| [2021](https://arxiv.org/abs/2106.12574) | DeepStochLog | Neural Stochastic Logic Programming | [![arXiv 2021](https://img.shields.io/badge/arXiv-2021-brightgreen)](https://arxiv.org/abs/2106.12574) | [![Star](https://img.shields.io/github/stars/ML-KULeuven/deepstochlog.svg?style=social)](https://github.com/ML-KULeuven/deepstochlog) | <details><summary>Summary</summary> "DeepStochLog: Neural Stochastic Logic Programming" introduces a novel framework that combines neural networks with stochastic logic programming, offering a scalable and expressive approach to neural-symbolic computation. DeepStochLog enhances traditional logic programming with neural networks, improving its capabilities in complex tasks such as MNIST Digit Addition and Handwritten Mathematical Expressions. The framework not only demonstrates state-of-the-art performance in various neural-symbolic tasks but also excels in scalability and efficiency. DeepStochLog's ability to encode complex relational problems beyond standard grammars opens new possibilities in AI, highlighting its significance in advancing neural-symbolic integration. </details> |
| [2021](https://ieeexplore.ieee.org/document/9415044) | PLNN | Training Logical Neural Networks by Primal–Dual Methods for Neuro-Symbolic Reasoning | [![IEEE](https://img.shields.io/badge/Conference-IEEE'21-blue)](https://ieeexplore.ieee.org/document/9415044) | [![Star](https://img.shields.io/github/stars/songtaogithub/LNN.svg?style=social)](https://github.com/songtaogithub/LNN) | <details><summary>Summary</summary> "Training Logical Neural Networks by Primal–Dual Methods for Neuro-Symbolic Reasoning" explores the training of Logical Neural Networks (LNNs) under challenging constraints. The paper introduces a unified framework utilizing primal-dual optimization for this purpose, focusing on achieving convergence to KKT points in non-linear, nonconvex optimization scenarios. The proposed Inexact Alternating Direction Method of Multipliers (iADMM) effectively addresses the complexities in training LNNs by handling nonconvex inequality constraints. This advancement demonstrates superior results in training LNN models on real-world data sets, validating the approach's efficiency. The work sets a precedent for future research in optimizing LNNs, a vital tool for neuro-symbolic AI. </details> |
| [2021](https://arxiv.org/abs/2104.08400) | Structure-aware-BART | Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs | [![arXiv 2021](https://img.shields.io/badge/arXiv-2021-brightgreen)](https://arxiv.org/abs/2104.08400) | [![Star](https://img.shields.io/github/stars/SALT-NLP/Structure-Aware-BART.svg?style=social)](https://github.com/SALT-NLP/Structure-Aware-BART) | <details><summary>Summary</summary> "Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs" by Jiaao Chen and Diyi Yang introduces an innovative summarization model that integrates discourse relations and action graphs into conversation summaries. The model addresses the unstructured and complex nature of human conversations, improving the preciseness of summaries. By constructing discourse relation graphs and action graphs, the model effectively captures dependencies between utterances and associations between speakers and actions. Empirical tests show significant improvements over existing methods in both automated and human evaluations. This approach opens new avenues for accurate and informative conversation summarization in various domains. </details> |
| [2021](https://www.sciencedirect.com/science/article/pii/S2352711021001126) | 2P-Kt | A logic-based ecosystem for symbolic AI | [![ScienceDirect](https://img.shields.io/badge/Conference-ScienceDirect'21-blue)](https://www.sciencedirect.com/science/article/pii/S2352711021001126) | [![Star](https://img.shields.io/github/stars/tuProlog/2p-kt.svg?style=social)](https://github.com/tuProlog/2p-kt), [![Pika-Lab](https://img.shields.io/badge/Website-Pika--Lab-red)](https://pika-lab.gitlab.io/tuprolog/2p-kt-web/) | <details><summary>Summary</summary> "2P-Kt: A logic-based ecosystem for symbolic AI" by Giovanni Ciatto et al., presents 2P-Kt, a significant evolution of the tuProlog project, aimed at creating an open, modular, and interoperable ecosystem for logic programming and symbolic AI. 2P-Kt leverages Kotlin multiplatform technology, offering a wide range of functionalities such as logic term manipulation, unification, and solver engines. It supports different logic programming paradigms and facilitates the integration of symbolic and sub-symbolic AI. The platform is designed to be extensible, encouraging further developments in AI research and applications. 2P-Kt's open-source nature and comprehensive toolkit make it a valuable asset for both researchers and practitioners in AI. </details> |
| [2020](https://arxiv.org/abs/2012.13635) | LTN | Logic Tensor Networks | [![arXiv 2020](https://img.shields.io/badge/arXiv-2020-brightgreen)](https://arxiv.org/abs/2012.13635) | [![Star](https://img.shields.io/github/stars/logictensornetworks/logictensornetworks.svg?style=social)](https://github.com/logictensornetworks/logictensornetworks) | <details><summary>Summary</summary> Logic Tensor Networks (LTN) present a neurosymbolic AI framework, integrating logic and neural networks through a differentiable logical language, Real Logic. It grounds symbols from first-order logic onto data using neural networks, allowing rich knowledge representation and reasoning. LTN supports diverse AI tasks like classification, clustering, and regression. Learning involves parameter optimization to maximize formula satisfiability, while querying enables evaluation of truth values and inferences on new data. The framework includes methods for logical reasoning and addresses gradient issues through stable product real logic. Implemented in TensorFlow 2, LTN combines efficient computation with the expressiveness of first-order logic. </details> |
| [2020](https://arxiv.org/abs/1908.05646) | SenseBERT | Driving Some Sense into BERT | [![arXiv 2020](https://img.shields.io/badge/arXiv-2020-brightgreen)](https://arxiv.org/abs/1908.05646) | [![Star](https://img.shields.io/github/stars/AI21Labs/sense-bert.svg?style=social)](https://github.com/AI21Labs/sense-bert) | <details><summary>Summary</summary> SenseBERT, an extension of BERT, introduces a novel approach to lexical semantics by incorporating word sense information directly into the pre-training process. Using WordNet's supersense categories for weak-supervision, it enhances BERT's capability to understand and predict word meanings in context. This results in significantly improved performance on lexical tasks like Word Sense Disambiguation and the Word in Context (WiC) task, while maintaining competitive performance on standard benchmarks like GLUE. The model effectively manages word ambiguity and out-of-vocabulary words, showing potential for richer semantic learning in language models without relying on human-annotated data. SenseBERT represents a significant advancement in harnessing external linguistic knowledge sources for neural language models. </details> |
| [2019](https://arxiv.org/abs/1905.06088) | CILP | Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning | [![arXiv 2019](https://img.shields.io/badge/arXiv-2019-brightgreen)](https://arxiv.org/abs/1905.06088) | N/A | <details><summary>Summary</summary> "Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning" outlines an approach that merges neural network-based learning with symbolic logic-based reasoning, addressing the interpretability and accountability issues in AI. It covers different strategies for representing symbolic knowledge in neural networks and discusses the integration of various logic types for robust reasoning and learning. The paper highlights the importance of explainable AI, focusing on knowledge extraction, natural language generation, and program synthesis. It presents neural-symbolic learning and reasoning as key to developing intelligent systems that are not only efficient and effective but also interpretable and accountable. </details> |
| [2019](https://arxiv.org/abs/1910.13461) | BART (Meta) | Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | [![arXiv 2019](https://img.shields.io/badge/arXiv-2019-brightgreen)](https://arxiv.org/abs/1910.13461) | [![Star](https://img.shields.io/github/stars/facebookresearch/bart_ls.svg?style=social)](https://github.com/facebookresearch/bart_ls) | <details><summary>Summary</summary> "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension" introduces BART, a denoising autoencoder that pre-trains sequence-to-sequence models by corrupting and reconstructing text. It employs a Transformer-based architecture, combining aspects of BERT and GPT. BART's flexible noising strategy includes token masking, deletion, and text infilling, making it versatile for various tasks. It excels in text generation, comprehension tasks, and machine translation, matching or surpassing benchmark performances like RoBERTa's on GLUE and SQuAD. Ablation studies within the BART framework indicate its consistency across tasks. BART's scalability in pre-training shows significant potential for wide applications in natural language processing. </details> |
| [2018](https://arxiv.org/abs/1805.10872) | DeepProbLog | Neural Probabilistic Logic Programming | [![arXiv 2018](https://img.shields.io/badge/arXiv-2018-brightgreen)](https://arxiv.org/abs/1805.10872) | [![Star](https://img.shields.io/github/stars/ML-KULeuven/deepproblog.svg?style=social)](https://github.com/ML-KULeuven/deepproblog), [![DTAI](https://img.shields.io/badge/Website-DTAI-red)](https://dtai.cs.kuleuven.be/problog/) | <details><summary>Summary</summary> DeepProbLog introduces a novel approach to neural probabilistic logic programming, effectively integrating deep learning with probabilistic reasoning. It extends the probabilistic logic programming language ProbLog with neural predicates, handling uncertainty from neural network outputs within a logical reasoning framework. The language supports both symbolic and subsymbolic reasoning, allowing end-to-end model training on complex tasks that require more than just standard learning. Through gradient descent-based learning, DeepProbLog jointly trains parameters in logic programs and neural networks. This integration leverages the strengths of both neural networks and logical reasoning, making DeepProbLog a powerful tool for advanced artificial intelligence applications.  </details> |
| [2018](http://proceedings.mlr.press/v80/xu18h.html) | Sem-Loss Sym-Knowledge | A Semantic Loss Function for Deep Learning with Symbolic Knowledge | [![Conference](https://img.shields.io/badge/Conference-ML'18-blue)](http://proceedings.mlr.press/v80/xu18h.html) | [![Star](https://img.shields.io/github/stars/npepperUQLab/Knowledge-Based-Neural-Network-.svg?style=social)](https://github.com/npepperUQLab/Knowledge-Based-Neural-Network-) | <details><summary>Summary</summary> "A Semantic Loss Function for Deep Learning with Symbolic Knowledge" introduces a novel approach to integrate deep learning with symbolic logic through a semantic loss function. This function evaluates the conformity of neural network outputs to logical constraints, enhancing learning in both semi-supervised and structured prediction tasks. The method demonstrates significant improvements in classification accuracy on datasets like MNIST and CIFAR-10. It is particularly effective in complex structured prediction, such as preference rankings or path predictions. The semantic loss function is developed axiomatically, ensuring logical soundness and differentiability, and can be incorporated into standard deep learning models as an additional regularization term.  </details> |
| [2018](https://arxiv.org/abs/1810.04805) | BERT (Google) | Pre-training of Deep Bidirectional Transformers for Language Understanding | [![arXiv 2018](https://img.shields.io/badge/arXiv-2018-brightgreen)](https://arxiv.org/abs/1810.04805) | [![Star](https://img.shields.io/github/stars/google-research/bert.svg?style=social)](https://github.com/google-research/bert) | <details><summary>Summary</summary> BERT is a transformative language representation model that pre-trains deep bidirectional representations from the unlabeled text by considering the context from both sides of a token in all layers. This novel approach enables it to achieve remarkable results in eleven NLP tasks, surpassing existing benchmarks significantly. BERT operates through two pre-training tasks: masked LM and next sentence prediction, facilitating effective language model pre-training. Its architecture is a multi-layer bidirectional Transformer encoder, capable of handling both single and paired text inputs. This versatility, combined with minimal architectural adjustments for various downstream tasks, makes BERT exceptionally powerful and adaptable for a wide range of NLP applications. </details> |
| [2017](https://paperswithcode.com/task/amr-parsing) | AMR Parsing | Transition-based Neural Parser | [![Resource](https://img.shields.io/badge/Resource-PapersWithCode'23-blue)](https://paperswithcode.com/task/amr-parsing) | [![Star](https://img.shields.io/github/stars/IBM/transition-amr-parser.svg?style=social)](https://github.com/IBM/transition-amr-parser) | <details><summary>Summary</summary>This paper introduces a novel, efficient approach to parsing Abstract Meaning Representation (AMR) through an incremental, transition-based parser. Adapting techniques from dependency parsing, it addresses unique AMR challenges like non-projectivity, reentrancy, and complex word-to-node alignments. The parser processes sentences left-to-right in linear time, enhancing efficiency. The proposed evaluation metrics assess specific sub-tasks, providing deeper insights into parser performance across various aspects, including named entity recognition and negation handling. The parser shows competitive results, particularly in recovering named entities, despite not achieving the highest overall Smatch score, showcasing the potential for real-time applications and studies in building sentence meaning incrementally. </details> |
| [2014](https://content.iospress.com/articles/intelligenza-artificiale/ia106) | cplint | A framework for reasoning with Probabilistic Logic Programming | [![iris](https://img.shields.io/badge/IRIS-2014-Blue)](https://iris.unife.it/bitstream/11392/2368375/8/2017.riguzzi.cplint%20on%20SWISH_POST%20PRINT.pdf) | [![GitHub](https://img.shields.io/github/stars/friguzzi/cplint.svg?style=social)](https://github.com/friguzzi/cplint)  [![cplint](https://img.shields.io/badge/cplint-Website-red)](https://cplint.ml.unife.it/) | <details><summary>Summary</summary> "cplint on SWISH" is a cutting-edge web application enabling probabilistic logical inference directly in a web browser. It significantly simplifies user interaction by obviating the need for complex installations. The platform supports advanced features such as hybrid programs with both discrete and continuous variables, a unique offering in web-based probabilistic logic programming. It includes various inference algorithms like rejection sampling and Metropolis-Hasting for robust reasoning. Notably user-friendly, it provides a diverse array of examples across different probabilistic models, demonstrating the system's versatility and capability. Future enhancements promise to expand its functionalities, making it an increasingly powerful tool for probabilistic programming. </details> |
| [2007](https://www.sciencedirect.com/science/article/pii/S030439750600750X) | Modal | Connectionist modal logic: Representing modalities in neural networks | [![ScienceDirect](https://img.shields.io/badge/Conference-ScienceDirect'07-blue)](https://www.sciencedirect.com/science/article/pii/S030439750600750X) | [![Star](https://img.shields.io/github/stars/rkirsling/modallogic.svg?style=social)](https://github.com/rkirsling/modallogic) | <details><summary>Summary</summary> The paper "Connectionist Modal Logic: Representing Modalities in Neural Networks" introduces a novel framework that integrates neural networks and modal logic, referred to as Connectionist Modal Logic (CML). This approach is part of the broader domain of neural-symbolic integration, aiming to utilize symbolic knowledge within the neurocomputing paradigm. CML facilitates the representation and learning of propositional modal logic using neural networks. The framework is validated through the application to the Muddy Children Puzzle, demonstrating its potential in distributed knowledge representation. The paper suggests that CML offers a balance between expressive power and computational feasibility and hints at the possibility of extending beyond propositional logic to first-order logic. </details> |
| [2003](https://ieeexplore.ieee.org/document/1200733) | DAMLJessKB | A Tool for Reasoning with the Semantic Web | [![IEEE](https://img.shields.io/badge/Conference-IEEE'03-blue)](https://ieeexplore.ieee.org/document/1200733) | N/A | <details><summary>Summary</summary>"DAMLJessKB: A Tool for Reasoning with the Semantic Web" by Joseph Kopena and William C. Regli presents DAMLJessKB, an innovative tool designed for reasoning with Semantic Web technologies. The tool focuses on integrating Semantic Web standards like DAML for effective knowledge management and inference in domains such as engineering design. Using the Jess production system, DAMLJessKB interprets and processes RDF/DAML encoded data, enabling sophisticated reasoning capabilities, including class instance and terminological reasoning. The tool's practical applications, ease of integration, and potential enhancements to support OWL, make it a significant step towards realizing the Semantic Web vision. </details> |

## Knowledge representation major projects

| Publication Year | Name | Description | Paper Link | GitHub Link | Summary |
| ---------------- | ---- | ----------- | ---------- | ----------- | ------- |
| [2023](https://arxiv.org/abs/2305.06349) | ReckonMK | Reckoning MetaKG: Hyper-Edge Prediction in the Meta Knowledge Graph | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2305.06349) | [![GitHub](https://img.shields.io/github/stars/eric11eca/reckoning-metakg.svg?style=social)](https://github.com/eric11eca/reckoning-metakg) | <details><summary>Summary</summary>RECKONING introduces an innovative approach to enhance the reasoning abilities of transformer-based language models. It encodes contextual knowledge directly into the model's parameters using a bi-level optimization process, consisting of an inner loop for rapid adaptation and an outer loop for optimizing initial weights. This method improves robustness against distractors and enhances reasoning performance. Extensive testing on datasets like ProofWriter, CLUTRR-SG, and FOLIO shows that RECKONING outperforms traditional in-context reasoning, particularly in handling irrelevant facts and generalizing to longer reasoning chains and real-world knowledge. Additionally, it proves more efficient in scenarios involving multiple questions due to its single-time knowledge encoding.</details> |
| [2023](https://arxiv.org/abs/2310.15239) | Crow | Learning to Crowdsource Training Data for Few-Shot Learning | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2310.15239) | [![GitHub](https://img.shields.io/github/stars/mismayil/crow.svg?style=social)](https://github.com/mismayil/crow) | <details><summary>Summary</summary> The CROW benchmark innovatively evaluates commonsense reasoning in real-world tasks for NLP systems. Contrasting with artificial scenarios in existing datasets, CROW embeds commonsense-violating perturbations into six real-world tasks, including machine translation, open-domain dialogue, and safety detection. It employs a multi-stage data collection pipeline to rewrite examples from existing datasets, challenging models with nuanced reasoning tasks. The benchmark uses Macro-F1 and Situational Accuracy for evaluation, uncovering a significant gap between human performance and current NLP models. While CROW marks a significant advancement in commonsense reasoning benchmarks, it also acknowledges limitations like a narrow focus on commonsense dimensions and potential crowdsourcing biases. </details> |
| [2023](https://arxiv.org/abs/2305.02364) |  PeaCok |  Persona Commonsense Knowledge for Consistent and Engaging Narratives  | [![arXiv ](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2305.02364) |  [![GitHub](https://img.shields.io/github/stars/Silin159/PeaCoK.svg?style=social)](https://github.com/Silin159/PeaCoK) | <details><summary>Summary</summary>  PEACOK is a novel knowledge graph offering comprehensive persona commonsense knowledge to enhance narrative systems. Containing around 100K human-validated facts across five key dimensions, it was constructed using a combination of existing commonsense knowledge graphs and inputs from large-scale language models, refined through a human-AI majority voting process. This resource provides deep persona insights, fostering more consistent and engaging narratives in applications such as dialogue systems. While showcasing the potential of integrating machine-generated and human-validated content, PEACOK also acknowledges its linguistic and ethical limitations, primarily its focus on English data and reliance on existing knowledge sources. v</details> |
| [2023](https://bair.berkeley.edu/blog/2023/04/03/koala/) | KOALA | A Dialogue Model for Academic Research | [![BAIR Blog]https://img.shields.io/badge/Website-BAIR%20Blog-red)](https://bair.berkeley.edu/blog/2023/04/03/koala/) (blog post only) |  [![GitHub](https://img.shields.io/github/stars/young-geng/EasyLM.svg?style=social)](https://github.com/young-geng/EasyLM/blob/main/docs/koala.md) | <details><summary>Summary</summary> Koala is a new dialogue model developed by fine-tuning Meta's LLaMA with web-gathered dialogue data, aiming to match the capabilities of larger closed-source models. Focusing on high-quality, diverse data including user-shared dialogues with ChatGPT, Koala demonstrates competitive performance, sometimes preferred over Stanford’s Alpaca and equaling ChatGPT in user studies. This suggests smaller, local models with carefully curated data can achieve results similar to larger ones, highlighting the importance of dataset quality over size. However, Koala, being a research prototype, has limitations in content, safety, and reliability and is recommended only for academic research. Koala's release includes an interactive demo, training framework, model weights, and a test set, intended for academic use under specific licenses. The project, a Berkeley AI Research Lab (BAIR) initiative, encourages community feedback to identify potential improvements and safety issues. </details> |
| [2023](https://www.wolfram.com/data-framework/) | Wolfram Alpha  | Wolfram Data Framework (WDF) Take data and make it meaningful | [![Documentation](https://img.shields.io/badge/Documentation-Wolfram'23-blue)](https://reference.wolfram.com/language/guide/WDFWolframDataFramework.html) | [![GitHub](https://img.shields.io/github/stars/WolframResearch.svg?style=social)](https://github.com/WolframResearch) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://www.newyorker.com/culture/infinite-scroll/your-ai-companion-will-support-you-no-matter-what#:~:text=Now%2C%20with%20A.I.,personalities%20according%20to%20users'%20desires.) | AI companions | Various approaches from various companies | N/A | [![Kindroid](https://img.shields.io/badge/Website-Kindroid-red)](https://kindroid.ai/), [![Nomi](https://img.shields.io/badge/Website-Nomi-red)](https://nomi.ai/), [![Character](https://img.shields.io/badge/Website-Character-red)](https://beta.character.ai/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://arxiv.org/abs/2307.02628) | SKIP | Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference | [![arXiv 2023](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2307.02628) | [![GitHub](https://img.shields.io/github/stars/tuetschek/e2e-dataset.svg?style=social)](https://github.com/tuetschek/e2e-dataset) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://arxiv.org/abs/2208.01174) | TextWorldExpress | Simulating Text Games at One Million Steps Per Second | [![arXiv 2023](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2208.01174) | [![CognitiveAI Lab](https://img.shields.io/badge/Website-CognitiveAI_Lab-red)](https://github.com/cognitiveailab/TextWorldExpress), [![Microsoft](https://img.shields.io/badge/Website-Microsoft-red)](https://github.com/microsoft/TextWorld) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://github.com/facebookresearch/ParlAI) | ParlAI | Python framework for sharing, training and testing dialogue models, from open-domain chitchat | N/A (Meta) | [![GitHub](https://img.shields.io/github/stars/facebookresearch/ParlAI.svg?style=social)](https://github.com/facebookresearch/ParlAI), [![ParlAI](https://img.shields.io/badge/Website-ParlAI-red)](https://parl.ai/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://arxiv.org/abs/2305.19374) | NeuroConcept | Compositional diversity in visual concept learning | [![arXiv 2023](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2305.19374) | N/A yet | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://github.com/e-spaulding/xpo) | DWD overlay | The DARPA Wikidata Overlay | [![Paper](https://img.shields.io/badge/Conference-ISA'23-blue)](https://aclanthology.org/2023.isa-1.1.pdf) | [![GitHub](https://img.shields.io/github/stars/e-spaulding/xpo.svg?style=social)](https://github.com/e-spaulding/xpo) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://aclanthology.org/2023.acl-demo.1/) | HITL-Schema | Human-in-the-Loop Schema Induction | [![arXiv 2023](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2302.13048) | [![GitHub](https://img.shields.io/github/stars/aclanthology/2023.acl-demo.1.svg?style=social)](https://aclanthology.org/2023.acl-demo.1/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://ibm.github.io/neuro-symbolic-ai/toolkit/ulkb/) | ULKB  | Universal Logic Knowledge Base | N/A | [![GitHub](https://img.shields.io/github/stars/ibm/ulkb.svg?style=social)](https://github.com/ibm/ulkb) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://apps.dtic.mil/sti/trecms/pdf/AD1202435.pdf) | RAMFIS | REPRESENTATION OF VECTORS AND ABSTRACT MEANINGS FOR INFORMATION SYNTHESIS | [![Paper](https://img.shields.io/badge/Conference-Related'23-blue)](https://apps.dtic.mil/sti/trecms/pdf/AD1202435.pdf) | N/A (U.S. Air Force) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://arxiv.org/abs/2203.07540) | ScienceWorld | ScienceWorld: Is your Agent Smarter than a 5th Grader? | [![Paper](https://img.shields.io/badge/Paper-arXiv'22-brightgreen)](https://arxiv.org/abs/2203.07540) | [![GitHub](https://img.shields.io/github/stars/allenai/ScienceWorld.svg?style=social)](https://github.com/allenai/ScienceWorld) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://arxiv.org/abs/2210.07109) | D&D as a Dialogue | Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence | [![Paper](https://img.shields.io/badge/Paper-arXiv'22-brightgreen)](https://arxiv.org/abs/2210.07109) | N/A | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://arxiv.org/abs/2202.00120) | QALD | QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers | [![Paper](https://img.shields.io/badge/Paper-arXiv'22-brightgreen)](https://arxiv.org/abs/2202.00120) | [![GitHub](https://img.shields.io/github/stars/ag-sc/QALD.svg?style=social)](https://github.com/ag-sc/QALD) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://machinelearning.apple.com/research/continuous-construction) | SAGA | A Platform for Continuous Construction and Serving of Knowledge At Scale | [![Paper](https://img.shields.io/badge/Paper-arXiv'22-brightgreen)](https://arxiv.org/abs/2204.07309) | [![Apple](https://img.shields.io/badge/Website-Apple-red)](https://machinelearning.apple.com/research/continuous-construction) | <details><summary>Summary</summary>To be added</details> |
| [2022](https://www.ijcai.org/proceedings/2022/850) | Foresee | Knowledge-Based News Event Analysis and Forecasting Toolkit | [![Paper](https://img.shields.io/badge/Paper-IJCAI'22-blue)](https://www.ijcai.org/proceedings/2022/850) | N/A | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://soargroup.github.io/rosie/) | ROSIE (from SOAR) | Rosie (RObotic Soar Instructable Entity) is an agent written in the Soar Cognitive Architecture | [![Website](https://img.shields.io/badge/Website-See_Details-blue)](https://soar.eecs.umich.edu/) | [![GitHub](https://img.shields.io/github/stars/SoarGroup/rosie.svg?style=social)](https://github.com/SoarGroup/rosie) |  <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2018-22](https://soar.eecs.umich.edu/) | SOAR | Soar Cognitive Architecture | [![Website](https://img.shields.io/badge/Website-See_Details-blue)](https://soar.eecs.umich.edu/) | [![GitHub](https://img.shields.io/github/stars/SoarGroup/Soar.svg?style=social)](https://github.com/SoarGroup/Soar) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2021](https://arxiv.org/abs/2110.10963) | Neuro-Symbolic RL | Neuro-Symbolic Reinforcement Learning with First-Order Logic | [![Paper](https://img.shields.io/badge/Paper-arXiv'21-brightgreen)](https://arxiv.org/abs/2110.10963) | [![Toolkit](https://img.shields.io/badge/Toolkit-IBM-red)](https://ibm.github.io/neuro-symbolic-ai/toolkit/) | <details><summary>Summary</summary>To be added</details> |
| [2021](https://aclanthology.org/2021.emnlp-main.245/) | Neuro-Symbolic learning | Neuro-Symbolic Approaches for Text-Based Policy Learning | [![Paper](https://img.shields.io/badge/Paper-EMNLP'21-blue)](https://aclanthology.org/2021.emnlp-main.245/) | N/A | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2021](https://arxiv.org/abs/2104.08811) | Human Schema | Human Schema Curation via Causal Association Rule Mining | [![Paper](https://img.shields.io/badge/Paper-arXiv'21-brightgreen)](https://arxiv.org/abs/2104.08811) | [![Links](https://img.shields.io/badge/Links-Inside_Paper-blue)](https://arxiv.org/abs/2104.08811) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2021](https://arxiv.org/abs/2101.00297) | Few Shot Comet | Analyzing Commonsense Emergence in Few-shot Knowledge Models | [![arXiv](https://img.shields.io/badge/arXiv-2021-brightgreen)](https://arxiv.org/abs/2101.00297) | [![GitHub](https://img.shields.io/github/stars/allenai/few-shot-comet.svg?style=social)](https://github.com/allenai/few-shot-comet) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2021](https://par.nsf.gov/servlets/purl/10288899) | UMR | Uniform Meaning Representation | [![Paper](https://img.shields.io/badge/Paper-NSF-blue)](https://par.nsf.gov/servlets/purl/10288899) | [![Web](https://img.shields.io/badge/Web-UMR4NLP-red)](https://umr4nlp.github.io/web/index.html), [![Tutorials (LREC)](https://img.shields.io/badge/Tutorials-LREC'21-red)](https://lrec2022.lrec-conf.org/en/workshops-and-tutorials/tutorials-details/), [![Tutorials (EMNLP)](https://img.shields.io/badge/Tutorials-EMNLP'21-red)](https://2022.emnlp.org/program/tutorials/) | <details><summary>Summary</summary>To be added</details> |
| [2021](https://www.akbc.ws/2021/papers/4TpJpZ-_gyl) | LLM Knowledge from Analogy | Combining Analogy with Language Models for Knowledge Extraction | [![Paper](https://img.shields.io/badge/Paper-AKBC'21-blue)](https://openreview.net/forum?id=4TpJpZ-_gyl) | [![GitHub](https://img.shields.io/github/stars/dnr2/analogical-ke.svg?style=social)](https://github.com/dnr2/analogical-ke) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2021](https://makg.org/) | MAKG/MS Satori |  Microsoft Academic Knowledge Graph / Satori project  | [![Web](https://img.shields.io/badge/Web-MAKG/MS-red)](https://makg.org/) [![Academic Webpage](https://img.shields.io/badge/Academic_Webpage-Microsoft-red)](https://www.microsoft.com/en-us/research/project/academic/) | [![GitHub](https://img.shields.io/github/stars/michaelfaerber/MAG2RDF.svg?style=social)](https://github.com/michaelfaerber/MAG2RDF) | <details><summary>Summary</summary>To be added</details> |
| [2020](https://ojs.aaai.org/index.php/AAAI/article/view/16792) | Comet Atomic | A Commonsense Knowledge Base for Modeling Atomic Events | [![arXiv](https://img.shields.io/badge/arXiv-2020-brightgreen)](https://ojs.aaai.org/index.php/AAAI/article/view/16792) | [![GitHub](https://img.shields.io/github/stars/allenai/comet-atomic-2020.svg?style=social)](https://github.com/allenai/comet-atomic-2020) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2020](https://arxiv.org/abs/2010.05953) | COMET-ATOMIC 2020 | On Symbolic and Neural Commonsense Knowledge Graphs | [![Paper](https://img.shields.io/badge/Paper-arXiv'20-brightgreen)](https://arxiv.org/abs/2010.05953) | [![GitHub](https://img.shields.io/github/stars/allenai/comet-atomic-2020.svg?style=social)](https://github.com/allenai/comet-atomic-2020) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2020](https://mosaickg.apps.allenai.org/kg_atomic2020) | Atomic | A commonsense knowledge graph with 1.33M everyday inferential knowledge tuples about entities and events | [![Paper](https://img.shields.io/badge/Paper-SemanticScholar'20-blue)](https://www.semanticscholar.org/paper/COMET-ATOMIC-2020%3A-On-Symbolic-and-Neural-Knowledge-Hwang-Bhagavatula/f8a22859230e0ccafefc020dccc66b5a646fe0ac) | [![GitHub](https://img.shields.io/github/stars/allenai/comet-atomic-2020.svg?style=social)](https://github.com/allenai/comet-atomic-2020/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2020](https://arxiv.org/abs/2010.03790) | TWC | Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines | [![Paper](https://img.shields.io/badge/Paper-arXiv'20-brightgreen)](https://arxiv.org/abs/2010.03790) | [![GitHub](https://img.shields.io/github/stars/IBM/commonsense-rl.svg?style=social)](https://github.com/IBM/commonsense-rl) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2020](https://arxiv.org/abs/2012.01707) | KBQA (IBM) | Leveraging Abstract Meaning Representation for Knowledge Base Question Answering | [![Paper](https://img.shields.io/badge/Paper-arXiv'20-brightgreen)](https://arxiv.org/abs/2012.01707) | [![GitHub](https://img.shields.io/github/stars/IBM/kbqa-relation-linking.svg?style=social)](https://github.com/IBM/kbqa-relation-linking) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2020](https://aclanthology.org/2020.acl-main.120/) | WCEP | A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal | [![Paper](https://img.shields.io/badge/Paper-ACL'20-blue)](https://aclanthology.org/2020.acl-main.120/) | [![GitHub](https://img.shields.io/github/stars/complementizer/wcep-mds-dataset.svg?style=social)](https://github.com/complementizer/wcep-mds-dataset) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2019](https://www.darpa.mil/program/knowledge-directed-artificial-intelligence-reasoning-over-schemas) | KAIROS | Knowledge-directed Artificial Intelligence Reasoning Over Schemas | N/A | [![DARPA](https://img.shields.io/badge/DARPA-KAIROS-red)](https://www.darpa.mil/program/knowledge-directed-artificial-intelligence-reasoning-over-schemas) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2019](https://arxiv.org/abs/1911.02060) | KBTE | Infusing Knowledge into the Textual Entailment Task Using Graph Convolutional Networks | [![Paper](https://img.shields.io/badge/Paper-arXiv'19-brightgreen)](https://arxiv.org/abs/1911.02060) | [![GitHub](https://img.shields.io/github/stars/IBM/knowledge-enabled-textual-entailment.svg?style=social)](https://github.com/IBM/knowledge-enabled-textual-entailment) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2019](https://www.arxiv-vanity.com/papers/1906.05317/) | COMET | Commonsense Transformers for Automatic Knowledge Graph Construction | [![Paper](https://img.shields.io/badge/Paper-arXiv'19-brightgreen)](https://arxiv.org/abs/1906.05317) | [![GitHub](https://img.shields.io/github/stars/atcbosselut/comet-commonsense.svg?style=social)](https://github.com/atcbosselut/comet-commonsense) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2018](https://github.com/microsoft/jericho) | Jericho | A lightweight python-based interface connecting learning agents with interactive fiction games | N/A | [![GitHub](https://img.shields.io/github/stars/microsoft/jericho.svg?style=social)](https://github.com/microsoft/jericho) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2018](https://api.semanticscholar.org/CorpusID:202558974) | ProPara | Comprehension of simple paragraphs describing processes | [![Paper](https://img.shields.io/badge/Paper-SemanticScholar'18-blue)](https://api.semanticscholar.org/CorpusID:202558974) | [![AllenAI](https://img.shields.io/badge/AllenAI-ProPara-red)](https://allenai.org/data/propara) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2017](https://www.nas.gov.ua/text/EuropeanIntegration/HR001117S0026%20(AIDA).pdf) | DARPA AIDA | Active Interpretation of Disparate Alternatives | [![Paper](https://img.shields.io/badge/Paper-DARPA'17-blue)](https://www.nas.gov.ua/text/EuropeanIntegration/HR001117S0026%20(AIDA).pdf) | [![DARPA](https://img.shields.io/badge/DARPA-AIDA-red)](https://www.darpa.mil/program/active-interpretation-of-disparate-alternatives) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2016](https://arxiv.org/abs/1612.03975) | ConceptNet (5.5) | An Open Multilingual Graph of General Knowledge | [![Paper](https://img.shields.io/badge/Paper-arXiv'16-brightgreen)](https://arxiv.org/abs/1612.03975) | [![GitHub](https://img.shields.io/github/stars/commonsense/conceptnet5.svg?style=social)](https://github.com/commonsense/conceptnet5/), [![ConceptNet](https://img.shields.io/badge/Website-ConceptNet-red)](https://conceptnet.io/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2014](https://dl.acm.org/doi/10.1145/2629489) | WikiData | A free collaborative knowledgebase | [![Paper](https://img.shields.io/badge/Paper-ACM'14-blue)](https://dl.acm.org/doi/10.1145/2629489) | [![GitHub](https://img.shields.io/github/stars/Wikidata.svg?style=social)](https://github.com/Wikidata) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2012](https://en.wikipedia.org/wiki/Google_Knowledge_Graph) | Google KG | The Google Knowledge Graph | [![Website](https://img.shields.io/badge/Website-Wikipedia-red)](https://en.wikipedia.org/wiki/Google_Knowledge_Graph) | N/A | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2010](https://coggle.it/diagram/Wlcm4xImdAABI9yH/t/watson-ibm-knowledge-base-prismatic) | PRISMATIC | Inducing knowledge from a large scale lexicalized relation resource | [![Paper](https://img.shields.io/badge/Paper-ACM'10-blue)](https://dl.acm.org/doi/10.5555/1866775.1866790) | N/A | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2007](https://academic.oup.com/bioinformatics/article/23/14/1846/190290) | GeoQuery | A bridge between the Gene Expression Omnibus (GEO) and BioConductor | [![Paper](https://img.shields.io/badge/Paper-Bioinformatics'07-blue)](https://academic.oup.com/bioinformatics/article/23/14/1846/190290) | [![GitHub](https://img.shields.io/github/stars/seandavi/GEOquery.svg?style=social)](https://github.com/seandavi/GEOquery) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |

<!-- <img width="580" alt="image" src="https://github.com/Brandonio-c/NeuroAI-Cognition-Hub/assets/76982807/86ac0431-b47a-4582-bdbc-4e7789bbf7d4"> -->

## Cognitive Architectures and Generative Models

| Publication Year | Name | Description | Paper Link | GitHub Link | Summary |
| ---------------- | ---- | ----------- | ---------- | ----------- | ------- |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27710) | LLMs in ACT-R/Soar | Comparing LLMs for enhanced ACT-R and Soar Model Development. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27710) | N/A | <details><summary>Summary</summary>Examines ChatGPT4 and Google Bard in cognitive task development, proposing a framework for LLM interaction.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27690) | LLMs for Cognitive Agents | Using Language Models as knowledge sources for cognitive agents. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27690) | N/A | <details><summary>Summary</summary>Explores the potential of LLMs in knowledge extraction and integration with cognitive architecture.</details> |
| [2024](https://www.qrg.northwestern.edu/papers/Files/QRG_Dist_Files/QRG_2023/nakos_forbus_fss23_aaai_camera_ready.pdf) | LLM Cog Companion | [Using Large Language Models in the Companion Cognitive Architecture: A Case Study and Future Prospects | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://www.qrg.northwestern.edu/papers/Files/QRG_Dist_Files/QRG_2023/nakos_forbus_fss23_aaai_camera_ready.pdf) | N/A | <details><summary>Summary</summary>To be added</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27683) | Chunks in Cognitive Arch | Method for generating knowledge chunks in cognitive architectures. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27683) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/w3c/cogai/blob/master/chunks-and-rules.md) | <details><summary>Summary</summary>Introduces NLP-based knowledge chunk creation for enhancing cognitive architecture knowledge bases.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27684) | Generative Models in CA | Integrating generative models in cognitive architecture for agents. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27684) | N/A | <details><summary>Summary</summary>Discusses the integration of generative AI techniques with cognitive architectures, focusing on the ICARUS cognitive architecture.</details> |
| [2024](https://arxiv.org/abs/2401.17464) | CoA-Reasoning | Efficient Tool Use with Chain-of-Abstraction Reasoning | [![Paper](https://img.shields.io/badge/Paper-arXiv'24-brightgreen)](https://arxiv.org/abs/2401.17464) | N/A | <details><summary>Summary</summary>To be added</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27686) | VSA | Bridging Cognitive Architectures and Generative Models with Vector Symbolic Algebras. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27686) | N/A | <details><summary>Summary</summary>Discusses integrating cognitive architectures and generative models using Vector Symbolic Algebras.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27687) | ML & CS | Building Intelligent Systems by Combining Machine Learning and Automated Commonsense Reasoning. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27687) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/aravindvaddi/chatbot-concierge) | <details><summary>Summary</summary>Proposes combining machine learning and automated commonsense reasoning.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27689) | LLM & CA | Augmenting Cognitive Architectures with Large Language Models. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27689) | N/A | <details><summary>Summary</summary>Explores integrating LLMs with cognitive architectures like Soar and Sigma.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27690) | LLM-CA | Exploiting Language Models as a Source of Knowledge for Cognitive Agents. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27690) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/langchain-ai/langchain) | <details><summary>Summary</summary>Examines the use of LLMs as a knowledge source for cognitive agents.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27691) | LMCA | A Proposal for a Language Model Based Cognitive Architecture. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27691) | N/A | <details><summary>Summary</summary>Proposes an architecture combining LLMs with cognitive architectures.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27692) | CA & Trans | Proposal for Cognitive Architecture and Transformer Integration. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27692) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/facebookresearch/gtn_applications) | <details><summary>Summary</summary>Discusses integrating Transformers with cognitive architectures for online learning.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27693) | Minds&Mach | Combining Minds and Machines: Fusion of Cognitive Architectures and Generative Models. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27693) | N/A | <details><summary>Summary</summary>Explores integrating cognitive architectures and generative models for embodied intelligence.</details> |
[2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27694) | EmbodGen | Growing an Embodied Generative Cognitive Agent. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27694) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/charles-river-analytics/Scruff.jl) | <details><summary>Summary</summary>Discusses developing cognitive architectures integrated with generative models, focusing on embodiment and deeper cognition.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27695) | Grounding | The Grounding Problem: Integration of Cognitive and Generative Models. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27695) | N/A | <details><summary>Summary</summary>Addresses the 'grounding' issue in integrating cognitive and neural AI systems.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27696) | GenInstLearn | Generative Environment-Representation Instance-Based Learning: A Cognitive Model. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27696) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/mighty-gerbils/gerbil) | <details><summary>Summary</summary>Explores Instance-Based Learning Theory using generative models for dynamic decision-making tasks.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27697) | LLM-IBL | Exploring Instructions to Rewards with LLMs in Instance-Based Learning. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27697) | N/A | <details><summary>Summary</summary>Discusses using LLMs to enhance learning from experience by converting instructions into dense signals in an Instance-Based Learning model.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27698) | GenAgents | Psychologically-Valid Generative Agents in Agent-Based Modeling. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27698) | N/A | <details><summary>Summary</summary>Introduces a framework for dynamic, realistic human behaviors in computational models, combining Cognitive Architecture and LLMs.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27699) | CG-AI | Cognitive Architecture for Common Ground Sharing in Model-Model Interaction. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27699) | N/A | <details><summary>Summary</summary>Explores model-model communication for common ground, using tangram naming tasks to test metaphorical image construction and interpretation.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27700) | LLM-Comp | Using LLMs in the Companion Cognitive Architecture. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27700) | N/A | <details><summary>Summary</summary>Outlines integrating BERT with the Companion cognitive architecture to enhance natural language capabilities.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27701) | Neuro-Cog | High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27701) | N/A | <details><summary>Summary</summary>Discusses integrating cognitive architectures with neuro-symbolic components for high-level reasoning in AI systems.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27703) | Auto-Know | Automating Knowledge Acquisition with LLMs for Cognitive Agents. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27703) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/uwnlp/nlp-corpora/tree/master/doc/byu-coca) | <details><summary>Summary</summary>Describes a system using LLMs for automatic learning in an intelligent agent's semantic lexicon.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27704) | SecurSoar | Proposed Uses of Generative AI in a Cybersecurity-Focused Soar Agent. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27704) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/aica-iwg/aica-agent/) | <details><summary>Summary</summary>Explores integrating generative AI into Soar for cybersecurity, enhancing analysis and learning in novel situations.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27705) | LeverageConf | Leveraging Conflict to Bridge Cognitive Reasoning and Generative Algorithms. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27705) | N/A | <details><summary>Summary</summary>Discusses a framework using conflict detection for balancing commonsense and deliberative reasoning in novel situations.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27706) | Synerg-LLM | Integrating LLMs and Cognitive Architectures for Robust AI. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27706) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/Significant-Gravitas/Auto-GPT) | <details><summary>Summary</summary>Discusses three approaches to integrating LLMs and Cognitive Architectures for developing robust AI systems.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27707) | GenCMC | Applying Generative Methods to the Common Model of Cognition. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27707) | N/A | <details><summary>Summary</summary>Explores the use of generative methods like DCM for validating the CMC's network structure.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27710) | LLM-ACT-R | LLMs for Prompt-Enhanced ACT-R and Soar Model Development. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27710) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/f/awesome-chatgpt-prompts) | <details><summary>Summary</summary>Studies the use of ChatGPT4 and Google Bard in ACT-R/Soar model development.</details> |
| [2023](https://arxiv.org/abs/2309.02427) | CoALA | Cognitive Architectures for Language Agents | [![Paper](https://img.shields.io/badge/Paper-arXiv'23-brightgreen)](https://arxiv.org/abs/23002427) | [![GitHub](https://img.shields.io/github/stars/ysymyth/awesome-language-agents.svg?style=social)](https://github.com/ysymyth/awesome-language-agents) | <details><summary>Summary</summary>To be added</details> |
| [2023](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27703) | Auto-knowledge-LLM | Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using LLMs | [![Paper](https://img.shields.io/badge/Paper-arXiv'23-brightgreen)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27703) | N/A | <details><summary>Summary</summary>To be added</details> |
| [2023](https://arxiv.org/abs/2308.09830) | Symbolic LLM's | Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis | [![Paper](https://img.shields.io/badge/Paper-arXiv'23-brightgreen)](https://arxiv.org/abs/2308.09830) | N/A | <details><summary>Summary</summary>To be added</details> |
| [2022](https://arxiv.org/abs/2201.09305) | ACT-R VS Soar | An Analysis and Comparison of ACT-R and Soar | [![Paper](https://img.shields.io/badge/Paper-arXiv'22-brightgreen)](https://arxiv.org/abs/2201.09305) | N/A | <details><summary>Summary</summary>To be added</details> |
| [1989](https://www.sciencedirect.com/science/article/abs/pii/0004370289900775) | Structure Mapping Engine | A computational model for analogical reasoning based on psychological theory. | [![Paper](https://img.shields.io/badge/Paper-ScienceDirect'89-blue)](https://www.sciencedirect.com/science/article/abs/pii/0004370289900775) | N/A | <details><summary>Summary</summary>Overview of SME's role in problem-solving and concept comprehension through analogy.</details> |

## Common Model of Cognition

| Publication Year | Name | Description | Paper Link | GitHub Link | Summary |
| ---------------- | ---- | ----------- | ---------- | ----------- | ------- |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27707) | Generative Methods CMC | Exploring generative methods in the Common Model of Cognition. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27707) | N/A | <details><summary>Summary</summary>Discusses using Dynamic Causal Modeling for validating CMC, highlighting new methods for model complexity.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27702) | COG-GEN | Neuro-Mimetic Realization of the Common Model of Cognition. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27702) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/NACLab/ngc-learn) | <details><summary>Summary</summary>Introduces COGnitive Neural GENerative system for representing the Common Model of Cognition using Hebbian learning and free energy principle.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27708) | Cog-FM | Integrating Cognitive Architectures with Foundation Models. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27708) | N/A | <details><summary>Summary</summary>Discusses integrating cognitive architectures with foundation models for few-shot learning.</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27709) | BridgeGen | Bridging Generative Networks with the Common Model of Cognition. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27709) | N/A | <details><summary>Summary</summary>Presents a framework for adapting the CMC to large generative network models in AI.</details> |

## Memory AI major projects

| Publication Year | Name | Description | Paper Link | GitHub Link | Summary |
| ---------------- | ---- | ----------- | ---------- | ----------- | ------- |
| [2022](https://openreview.net/forum?id=ZVBtN6B_6i7) | Learn2Expire | Learning to Expire: Synthesizing Lifelike Influence Expiry Times for Transformer Recommendations | [![arXiv](https://img.shields.io/badge/arXiv-2022-brightgreen)](https://openreview.net/forum?id=ZVBtN6B_6i7) | [![GitHub](https://img.shields.io/github/stars/lucidrains/learning-to-expire-pytorch.svg?style=social)](https://github.com/lucidrains/learning-to-expire-pytorch) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2024](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27688) | LLM-Memory | Memory Matters: The Need to Improve Long-Term Memory in LLM-Agents. | [![Paper](https://img.shields.io/badge/Paper-AAAI'24-blue)](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27688) | [![GitHub](https://img.shields.io/github/stars/.svg?style=social)](https://github.com/jina-ai/dev-gpt279) | <details><summary>Summary</summary>Reviews current efforts in developing LLM agents with a focus on improving long-term memory using vector databases. Highlights challenges and proposes future research topics.</details> |



## Meta-level control major projects

| Publication Year | Name | Description | Paper Link | GitHub Link | Summary |
| ---------------- | ---- | ----------- | ---------- | ----------- | ------- |
| [2023](https://groundhog-shallot-pen6.squarespace.com/explicable-insights) | CORA | Elemental AI Cognition product | N/A | [![Website](https://img.shields.io/badge/Website-CORA-red)](https://groundhog-shallot-pen6.squarespace.com/explicable-insights) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://direct.mit.edu/pages/cognet) | MIT COGENT | Elemental AI Cognition product | [![Conference](https://img.shields.io/badge/Conference-MITECS-blue)](https://direct.mit.edu/books/edited-volume/5452/The-MIT-Encyclopedia-of-the-Cognitive-Sciences) | [![Website](https://img.shields.io/badge/Website-MIT_CogNet-red)](https://direct.mit.edu/pages/cognet) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://arxiv.org/abs/2011.13354) | BRAID | Weaving Symbolic and Neural Knowledge into Coherent Logical Explanations | [![Paper](https://img.shields.io/badge/Paper-ArXiv'22-brightgreen)](https://arxiv.org/abs/2011.13354) | [![GitHub](https://img.shields.io/github/stars/ec-ai/braid.svg?style=social)](https://ec.ai/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://arxiv.org/abs/2205.07830) | FactPEGASUS | Factuality-Aware Pre-training and Fine-tuning for Abstractive Summarization | [![Paper](https://img.shields.io/badge/Paper-ArXiv'22-brightgreen)](https://arxiv.org/abs/2205.07830) | [![GitHub](https://img.shields.io/github/stars/meetdavidwan/factpegasus.svg?style=social)](https://github.com/meetdavidwan/factpegasus) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://arxiv.org/abs/2204.06508) | FactGraph | FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations | [![Paper](https://img.shields.io/badge/Paper-ArXiv'22-brightgreen)](https://arxiv.org/abs/2204.06508) | [![GitHub](https://img.shields.io/github/stars/amazon-science/fact-graph.svg?style=social)](https://github.com/amazon-science/fact-graph) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://arxiv.org/abs/2210.01240) | PrOntoQa | A Systematic Formal Analysis of Chain-of-Thought | [![Paper](https://img.shields.io/badge/Paper-ArXiv'22-brightgreen)](https://arxiv.org/abs/2210.01240) | [![GitHub](https://img.shields.io/github/stars/asaparov/prontoqa.svg?style=social)](https://github.com/asaparov/prontoqa) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2021](https://arxiv.org/abs/2110.01834) | SOFAI | Thinking Fast and Slow in AI: the Role of Metacognition | [![Paper](https://img.shields.io/badge/Paper-ArXiv'21-brightgreen)](https://arxiv.org/abs/2110.01834) | [![Website](https://img.shields.io/badge/Website-SOFAI-red)](https://sites.google.com/view/sofai/home) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2021](https://arxiv.org/abs/2010.10597) | SKATE | A Natural Language Interface for Encoding Structured Knowledge | [![Paper](https://img.shields.io/badge/Paper-ArXiv'21-brightgreen)](https://arxiv.org/abs/2010.10597) | [![Website](https://img.shields.io/badge/Website-SKATE-red)](https://ec.ai/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2020](https://arxiv.org/abs/2009.07758) | GLUCOSE | GeneraLized and COntextualized Story Explanations | [![Paper](https://img.shields.io/badge/Paper-ArXiv'20-brightgreen)](https://arxiv.org/abs/2009.07758) | [![Website](https://img.shields.io/badge/Website-GLUCOSE-red)](https://ec.ai/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |



## Benchmarks 

| Publication Year | Name | Description | Paper Link | GitHub Link | Summary |
| ---------------- | ---- | ----------- | ---------- | ----------- | ------- |
| [2023](https://github.com/Silin159/ComFact) | ComFact | A Benchmark for Linking Contextual Commonsense Knowledge | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://github.com/Silin159/ComFact) | [![GitHub](https://img.shields.io/github/stars/Silin159/ComFact.svg?style=social)](https://github.com/Silin159/ComFact) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://arxiv.org/abs/2310.15239) | Crow |Benchmarking Commonsense Reasoning in Real-World Tasks | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2310.15239) | [![GitHub](https://img.shields.io/github/stars/mismayil/crow.svg?style=social)](https://github.com/mismayil/crow) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://arxiv.org/abs/2209.06293)  | CaptionCon | A corpus for caption generation models | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2209.06293) | [![GitHub](https://img.shields.io/github/stars/jmhessel/caption_contest_corpus.svg?style=social)](https://github.com/jmhessel/caption_contest_corpus) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://ceur-ws.org/Vol-3342/paper-8.pdf) | Causal Relation Benchmark | Knowledge Graph Embeddings for Causal Relation Prediction | [![Paper](https://img.shields.io/badge/Conference-CEUR_WS-blue)](https://ceur-ws.org/Vol-3342/paper-8.pdf) | [![Zenodo](https://img.shields.io/github/stars/your-github-username/your-repository.svg?style=social)](https://zenodo.org/records/7195904) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://arxiv.org/abs/2309.12423) | Wikidata Causal Event Triple Data | Event Prediction using Case-Based Reasoning over Knowledge Graphs | [![arXiv](https://img.shields.io/badge/arXiv-2022-brightgreen)](https://arxiv.org/abs/2309.12423) | [![Zenodo](https://img.shields.io/github/stars/your-github-username/your-repository.svg?style=social)](https://zenodo.org/records/7196049) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2022](https://arxiv.org/abs/2206.06192) | Switchboard | Collection of Recognition of conversational speech over telephone | [![arXiv](https://img.shields.io/badge/arXiv-2022-brightgreen)](https://arxiv.org/abs/2206.06192) | [![Benchmarks.AI](https://img.shields.io/badge/Website-Benchmarks.AI-red)](https://benchmarks.ai/switchboard) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2019](https://aclanthology.org/N19-1072/) | LitePyramid | Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation | [![Paper](https://img.shields.io/badge/Conference-ACL'19-blue)](https://aclanthology.org/N19-1072/) | [![GitHub](https://img.shields.io/github/stars/OriShapira/LitePyramids.svg?style=social)](https://github.com/OriShapira/LitePyramids) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2019](https://arxiv.org/abs/1804.07461) | GLUE | General Language Understanding Evaluation (GLUE) benchmark | [![arXiv](https://img.shields.io/badge/arXiv-2019-brightgreen)](https://arxiv.org/abs/1804.07461) | [![GLUE Benchmark](https://img.shields.io/badge/Website-GLUE_Benchmark-red)](https://gluebenchmark.com/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2019](https://arxiv.org/abs/1908.06177) | CLUTRR (Meta) | A Diagnostic Benchmark for Inductive Reasoning from Text | [![arXiv](https://img.shields.io/badge/arXiv-2019-brightgreen)](https://arxiv.org/abs/1908.06177) | [![GitHub](https://img.shields.io/github/stars/facebookresearch/clutrr.svg?style=social)](https://github.com/facebookresearch/clutrr) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2016](https://arxiv.org/abs/1606.05250) | SQuAD | 100,000+ Questions for Machine Comprehension of Text | [![arXiv](https://img.shields.io/badge/arXiv-2016-brightgreen)](https://arxiv.org/abs/1606.05250) | [![GitHub](https://img.shields.io/github/stars/rajpurkar/SQuAD-explorer.svg?style=social)](https://rajpurkar.github.io/SQuAD-explorer/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2012](https://arxiv.org/abs/1207.4708) | Atari | The Arcade Learning Environment: An Evaluation Platform for General Agents | [![arXiv](https://img.shields.io/badge/arXiv-2012-brightgreen)](https://arxiv.org/abs/1207.4708) | [![GitHub](https://img.shields.io/github/stars/Farama-Foundation/Arcade-Learning-Environment.svg?style=social)](https://github.com/Farama-Foundation/Arcade-Learning-Environment) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2012](https://ieeexplore.ieee.org/document/6296535) | MNIST | Database of Handwritten Digit Images for Machine Learning Research | [![Paper](https://img.shields.io/badge/Conference-IEEE-blue)](https://ieeexplore.ieee.org/document/6296535) | [![MNIST](https://img.shields.io/badge/Website-MNIST-red)](http://yann.lecun.com/exdb/mnist/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2009](https://ieeexplore.ieee.org/document/5206848) | ImageNet | Image database organized according to the WordNet hierarchy (currently only the nouns) | [![Paper](https://img.shields.io/badge/Conference-IEEE-blue)](https://ieeexplore.ieee.org/document/5206848) | [![ImageNet](https://img.shields.io/badge/Website-ImageNet-red)](https://www.image-net.org/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |


## Generative AI Impactful Projects

| Publication Year | Name | Description | Paper Link | GitHub Link | Summary |
| ---------------- | ---- | ----------- | ---------- | ----------- | ------- |
| [2024](https://arxiv.org/abs/2401.17464) | CoA Reasoning tool | Efficient Tool Use with Chain-of-Abstraction Reasoning | [![arXiv](https://img.shields.io/badge/arXiv-2024-brightgreen)](https://arxiv.org/abs/2401.17464) | N/A | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://github.com/facebookresearch/fairseq/) | Fairseq (Meta) | Toolkit for LLM's | various (see GitHub page) | [![GitHub](https://img.shields.io/github/stars/facebookresearch/fairseq.svg?style=social)](https://github.com/facebookresearch/fairseq/) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2023](https://arxiv.org/abs/2203.17189) | T5X | Toolkit for LLM's | [![arXiv](https://img.shields.io/badge/arXiv-2023-brightgreen)](https://arxiv.org/abs/2203.17189) | [![GitHub](https://img.shields.io/github/stars/google-research/t5x.svg?style=social)](https://github.com/google-research/t5x) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2020](https://aclanthology.org/2020.acl-main.463/) | Octopus Paper | Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data | [![Conference](https://img.shields.io/badge/Conference-ACL'20-blue)](https://aclanthology.org/2020.acl-main.463/) | N/A | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |
| [2019](https://arxiv.org/abs/1910.10683) | T5 (Meta) | text to text transfer transformer | [![arXiv](https://img.shields.io/badge/arXiv-2019-brightgreen)](https://arxiv.org/abs/1910.10683) | [![GitHub](https://img.shields.io/github/stars/google-research/text-to-text-transfer-transformer.svg?style=social)](https://github.com/google-research/text-to-text-transfer-transformer) | <details><summary>Summary</summary>To be added<br>- To be added<br>- To be added</details> |



## Useful AI tools (2024) - There's literally an AI for everything

- [![Website](https://img.shields.io/badge/Website-CodeLlama-red)](https://codellama.dev/about) - Explore CodeLlama, a platform for LLM chatbots.
- [![Website](https://img.shields.io/badge/Website-OpenAI_ChatGPT-red)](https://openai.com/) - Learn about OpenAI's ChatGPT technology.
- [![Website](https://img.shields.io/badge/Website-Claude-red)](https://claude.ai/chats) - Discover Claude's AI chat capabilities.
- [![Website](https://img.shields.io/badge/Website-Bing_Chat_GPT4-red)](https://www.bing.com/search?q=Bing+AI&showconv=1) - Experience Bing Chat powered by GPT-4.
- [![Website](https://img.shields.io/badge/Website-Gemini_by_Google-red)](https://deepmind.google/technologies/gemini/#introduction) - Introduction to Gemini, Google's AI technology.

- [![Website](https://img.shields.io/badge/Website-2023_Generative_AI_Landscape_Link_1-red)](https://www.linkedin.com/pulse/generative-ai-landscape-2023-florian-belschner/) - "2023 Generative AI Landscape" on LinkedIn by Florian Belschner.

- [![Website](https://img.shields.io/badge/Website-2023_Generative_AI_Landscape_Link_2-red)](https://www.datacamp.com/cheat-sheet/the-generative-ai-tools-landscape) - "The Generative AI Tools Landscape" on DataCamp.

![generative AI landscape](https://github.com/Brandonio-c/NeuroAI-Cognition-Hub/assets/76982807/8c5de914-1b44-4c79-a416-d9b5de249acf)


<!--
(follow the link above to find a generative model listed under the subheading below) 

### Text Applications  

| Application space                                | Number of apps   |
| ------------------------------------------------ | -----------------|
| Search: Internet                                 | 8                |
| Search: Enterprise, Sales, Marketing & Accounting | 7                |
| Search: Jobs                                     | 5                |
| Search: Books, Images, Podcasts, Videos, & TV     | 9                |
| Search: Research                                  | 7                |
| Search: Programming/Software Development         | 5               |
| Chat                                              | 3              |
| Sales & Marketing Copy Generation                | 17               |
| Email Generation                                  | 8                |
| Other Copy Generation                             | 9               |
| Note-Taking & Document Summarization              | 6                |
| Writing Assistance & Translation                 | 3                |

### Image Applications

| Application space                                | Number of apps   |
| ------------------------------------------------ | -----------------|
| 2D Image Generation                              | 16              |
| Web Design, Color Palette Generation             | 6                |
| Image Editing, Enhancement & Style Transfer      | 7               |
| Ad Generation                                    | 5                |
| Presentations/Slide Generation                   | 10               |
| 3D Image Generation                              | 6                |
| Digital People Generation                        | 6                |

### Video Applications

| Application space                                | Number of apps   |
| ------------------------------------------------ | -----------------|
| Video Generation From Text                       | 6                |
| Video Editing                                    | 6               |
| Video Personalization & Derivative Content Generation | 7             |    

### Audio Applications

| Application space                                | Number of apps   |
| ------------------------------------------------ | -----------------|
| Music Generation                                 | 12               |
| Text to Speech                                   | 12               |

### Speech to Text (Transcription) 

| Application space                                | Number of apps   |
| ------------------------------------------------ | -----------------|
| Transcription: General                           | 10               |
| Transcription: Note Taking                       | 7                |
| Transcription: Subtitle Generation               | 6                |
| Transcription: Podcasts                          | 3               |
| Transcription: APIs                              | 2                 |
| Transcription: Other                             | 5                | 
| Dubbing                                          | 6               |
| Music Editing & Processing                       | 6                |
| Speech Editing & Processing                      | 5                |

### Coding Applications 

| Application space                                | Number of apps   |
| ------------------------------------------------ | -----------------|
| Website Generation from Text                     | 7                 | 
| Website Generation from Figma Designs            | 3                |
| Website Personalization & Optimization           | 4                 |
| Code Generation/Completion                       | 12                |
| Code Analysis & DevOps Intelligence              | 9                |
| Documentation Generation                         | 5                |

### Data Applications

| Application space                                | Number of apps   |
| ------------------------------------------------ | -----------------|
| Automated Analysis & Insights                    | 18              |
| Machine Learning & DataOps                       | 8                |
| Synthetic Training Data Generation               | 6                |

### Bots 

| Application space                                | Number of apps   |
| ------------------------------------------------ | -----------------|
| Chatbots                                         | 18               |
| Personal Assistants & AI Agents                  | 13               |
| Gaming                                           | 5                |
| Drug Development                                 | 4              |
| Language Learning                                |  11               |

### Miscellaneous 

| Application space                                | Number of apps   |
| ------------------------------------------------ | -----------------|
| Miscellaneous                                    | 6                |

-->


## Links to other useful GitHub pages

- [![Star](https://img.shields.io/github/stars/horseee/Awesome-Efficient-LLM.svg?style=social)](https://github.com/horseee/Awesome-Efficient-LLM) [Awesome-Efficient-LLM](https://github.com/horseee/Awesome-Efficient-LLM) - A comprehensive collection of resources on Efficient Large Language Models.
- [![Star](https://img.shields.io/github/stars/asahi417/AnalogyTools.svg?style=social)](https://github.com/asahi417/AnalogyTools) [AnalogyTools](https://github.com/asahi417/AnalogyTools) - Repository dedicated to tools for analogy-based learning and reasoning in AI.
- [![Star](https://img.shields.io/github/stars/totogo/awesome-knowledge-graph.svg?style=social)](https://github.com/totogo/awesome-knowledge-graph) [Awesome-Knowledge-Graph](https://github.com/totogo/awesome-knowledge-graph) - A curated list of awesome knowledge graph resources, papers, and tools.


## Usage

This repository primarily serves as a collection of links and references. You can explore the content by browsing the directories or using the search functionality on GitHub. Feel free to use the information here for your research, projects, or personal learning.

## Contributing

If you have valuable resources, papers, or articles related to neuro-symbolic AI and cognition within AI, we encourage you to contribute to this repository. Please follow the guidelines in our [Contribution Guidelines](CONTRIBUTING.md).

## License

This repository is open-source and is available under the [GNU Licence](LICENSE). Feel free to use and share the content while respecting the terms of the license.

We hope this repository helps you on your journey to understanding and exploring the exciting world of neuro-symbolic AI and cognition within AI. If you have any questions or suggestions, don't hesitate to [reach out](#contact-information).

Thank you for visiting!

---

### Contact Information

For inquiries or suggestions, please contact [Brandon Colelough](mailto:brandcol@umd.edu).
